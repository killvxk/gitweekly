#!/usr/bin/env python3
"""
å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥ç”Ÿæˆå·¥å…·
1. ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆåŸºäºgen_weekly.pyï¼‰
2. è‡ªåŠ¨è·å–GitHubå†…å®¹
3. ä½¿ç”¨AIç”Ÿæˆä¸­æ–‡æè¿°
4. æ›´æ–°å‘¨æŠ¥æ–‡ä»¶
"""
import re
import os
import json
import time
import subprocess
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# ============ é…ç½®åŒºåŸŸ ============
# AIæ¥å£é…ç½® - æ”¯æŒä¸¤ç§è®¤è¯æ–¹å¼
# 1. ANTHROPIC_API_KEY: ä¼ ç»ŸAPI Keyè®¤è¯ (x-api-key header)
# 2. ANTHROPIC_AUTH_TOKEN: OAuth Tokenè®¤è¯ (Authorization: Bearer header)
_BASE_URL = os.getenv("ANTHROPIC_BASE_URL", "https://api.anthropic.com")
# è‡ªåŠ¨å¤„ç†URLï¼šå¦‚æœä¸ä»¥/messagesç»“å°¾ï¼Œåˆ™è¿½åŠ  /v1/messages æˆ– /messages
if _BASE_URL.endswith("/messages"):
    AI_API_URL = _BASE_URL
elif _BASE_URL.endswith("/v1"):
    AI_API_URL = _BASE_URL.rstrip("/") + "/messages"
else:
    AI_API_URL = _BASE_URL.rstrip("/") + "/v1/messages"
AI_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
AI_AUTH_TOKEN = os.getenv("ANTHROPIC_AUTH_TOKEN", "")  # OAuth tokenæ¨¡å¼
AI_MODEL = os.getenv("AI_MODEL", "claude-sonnet-4-5-20250929")  # Claude Sonnet 4.5

# Gitä»“åº“é…ç½®
GIT_REPO_PATH = "f:/gitweekly"
WEEKLY_DIR = Path(GIT_REPO_PATH) / "weekly"
CACHE_DIR = Path(GIT_REPO_PATH) / "links_cache"
# ================================


class WeeklyGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨ - ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.weekly_dir = self.repo_path / "weekly"
        self.weekly_dir.mkdir(exist_ok=True)

        # æ–‡ä»¶ç±»å‹åˆ°åˆ†ç±»çš„æ˜ å°„
        self.category_map = {
            'README.md': 'ğŸ“¦ æ”¶é›†çš„é¡¹ç›®',
            'tools.md': 'ğŸ”§ æ”¶é›†çš„å·¥å…·',
            'BOF.md': 'ğŸ¯ BOFå·¥å…·',
            'skills-ai.md': 'ğŸ¤– AIä½¿ç”¨æŠ€å·§',
            'docs.md': 'ğŸ“š æ”¶é›†çš„æ–‡ç« ',
            'free.md': 'ğŸ å…è´¹èµ„æº'
        }

    def get_week_range(self, date_str: str) -> Tuple[str, str]:
        """è·å–æ—¥æœŸæ‰€åœ¨å‘¨çš„å‘¨ä¸€åˆ°å‘¨æ—¥"""
        date = datetime.strptime(date_str, "%Y-%m-%d")
        monday = date - timedelta(days=date.weekday())
        sunday = monday + timedelta(days=6)
        return monday.strftime("%Y-%m-%d"), sunday.strftime("%Y-%m-%d")

    def get_git_log(self, since_date: str = None) -> List[Dict]:
        """è·å–gitæäº¤æ—¥å¿—"""
        cmd = [
            'git', '-C', str(self.repo_path),
            'log', '--pretty=format:%H|%ad|%s',
            '--date=format:%Y-%m-%d'
        ]

        if since_date:
            cmd.extend(['--since', since_date])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )

        commits = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|', 2)
            if len(parts) == 3:
                commits.append({
                    'hash': parts[0],
                    'date': parts[1],
                    'message': parts[2]
                })

        return commits

    def extract_links_from_diff(self, commit_hash: str) -> Dict[str, List[Dict]]:
        """ä»æäº¤diffä¸­æå–é“¾æ¥ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç±»ï¼ˆæ’é™¤weeklyç›®å½•ï¼‰

        å¤„ç†é€»è¾‘ï¼š
        - GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
        - éGitHubé“¾æ¥ï¼šä½¿ç”¨æ ‡é¢˜è¡Œä½œä¸ºdesc
        """
        cmd = [
            'git', '-C', str(self.repo_path),
            'show', commit_hash, '--format=', '--unified=0'
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )

        links_by_file = defaultdict(list)
        current_file = None
        last_title = ""  # ä¿å­˜ä¸Šä¸€è¡Œçš„æ ‡é¢˜ï¼ˆç”¨äºéGitHubé“¾æ¥ï¼‰

        for line in result.stdout.split('\n'):
            # æ£€æµ‹æ–‡ä»¶å
            if line.startswith('diff --git'):
                match = re.search(r'b/(.+)$', line)
                if match:
                    current_file = match.group(1)
                    last_title = ""
                continue

            # åªå¤„ç†æ–°å¢çš„è¡Œ
            if not line.startswith('+') or line.startswith('+++'):
                continue

            # å»æ‰å¼€å¤´çš„+å·
            content = line[1:]

            # æ’é™¤weeklyç›®å½•ä¸‹çš„æ–‡ä»¶
            if not current_file or current_file not in self.category_map:
                continue
            if current_file.startswith('weekly/'):
                continue

            # æ£€æµ‹æ ‡é¢˜è¡Œ (#### æ ‡é¢˜ æˆ– ### æ ‡é¢˜ ç­‰)
            title_match = re.match(r'^#{1,6}\s+(.+)$', content)
            if title_match:
                last_title = title_match.group(1).strip()
                continue

            # 1. åŒ¹é…markdowné“¾æ¥æ ¼å¼: [text](https://...)
            markdown_pattern = r'\[([^\]]+)\]\((https?://[^\)]+)\)'
            markdown_matches = re.findall(markdown_pattern, content)
            for text, url in markdown_matches:
                if 'github.com' in url:
                    # GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
                    links_by_file[current_file].append({'url': url, 'desc': ''})
                else:
                    # éGitHubé“¾æ¥ï¼šä½¿ç”¨markdownä¸­çš„textä½œä¸ºdesc
                    links_by_file[current_file].append({'url': url, 'desc': text})

            # 2. åŒ¹é…çº¯URLæ ¼å¼: https://...
            markdown_urls = [m[1] for m in markdown_matches]
            url_pattern = r'https?://[^\s\)\]>]+'
            url_matches = re.findall(url_pattern, content)
            for url in url_matches:
                if url not in markdown_urls:
                    if 'github.com' in url:
                        # GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
                        links_by_file[current_file].append({'url': url, 'desc': ''})
                    else:
                        # éGitHubé“¾æ¥ï¼šä½¿ç”¨ä¸Šä¸€è¡Œçš„æ ‡é¢˜ä½œä¸ºdesc
                        links_by_file[current_file].append({'url': url, 'desc': last_title})
                    last_title = ""  # ä½¿ç”¨åæ¸…ç©º

            # å¦‚æœè¿™è¡Œä¸æ˜¯æ ‡é¢˜ä¹Ÿä¸æ˜¯é“¾æ¥ï¼Œæ¸…ç©ºlast_title
            if not title_match and not url_matches and not markdown_matches:
                if content.strip():  # éç©ºè¡Œ
                    last_title = ""

        return dict(links_by_file)

    def get_weekly_commits(self, week_start: str, week_end: str) -> Dict:
        """è·å–æŒ‡å®šå‘¨çš„æäº¤è®°å½•"""
        cmd = [
            'git', '-C', str(self.repo_path),
            'log', '--pretty=format:%H|%ad|%s',
            '--date=format:%Y-%m-%d',
            f'--since={week_start}',
            f'--until={week_end} 23:59:59'
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )

        commits = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|', 2)
            if len(parts) == 3:
                commits.append({
                    'hash': parts[0],
                    'date': parts[1],
                    'message': parts[2]
                })

        return {'commits': commits}

    def extract_links_from_diffs(self, commits: List[Dict]) -> Dict[str, List[Dict]]:
        """ä»å¤šä¸ªæäº¤çš„diffä¸­æå–é“¾æ¥ï¼ˆæ’é™¤weeklyç›®å½•ï¼‰"""
        all_links = defaultdict(list)

        for commit in commits:
            links_by_file = self.extract_links_from_diff(commit['hash'])
            for file, links in links_by_file.items():
                all_links[file].extend(links)

        return dict(all_links)

    def generate_markdown(self, week_start: str, week_end: str, weekly_data: Dict, links_by_file: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆå‘¨æŠ¥çš„markdownå†…å®¹"""
        content = f"# æœ¬å‘¨æ›´æ–° ({week_start} ~ {week_end})\n\n"

        # å»é‡å¹¶æŒ‰åˆ†ç±»ç»„ç»‡é“¾æ¥
        unique_links = {}
        for file, links in links_by_file.items():
            category = self.category_map.get(file, 'ğŸ“¦ å…¶ä»–')
            if category not in unique_links:
                unique_links[category] = {}
            # ä½¿ç”¨urlä½œä¸ºkeyå»é‡ï¼Œä¿ç•™desc
            for link in links:
                url = link['url']
                desc = link['desc']
                if url not in unique_links[category]:
                    unique_links[category][url] = desc
                elif not unique_links[category][url] and desc:
                    # å¦‚æœå·²æœ‰urlä½†æ²¡æœ‰descï¼Œæ›´æ–°desc
                    unique_links[category][url] = desc

        # æŒ‰åˆ†ç±»è¾“å‡º
        for category in sorted(unique_links.keys()):
            links_dict = unique_links[category]
            if links_dict:
                content += f"\n## {category}\n\n"
                content += "| é¡¹ç›® | è¯´æ˜ |\n"
                content += "|------|------|\n"

                for url, desc in links_dict.items():
                    # ä»URLæå–é¡¹ç›®å
                    name = url.rstrip('/').split('/')[-1]
                    # å¦‚æœdescæ˜¯é¡¹ç›®åæœ¬èº«ï¼Œæ¸…ç©ºå®ƒè®©AIç”Ÿæˆ
                    if desc and desc.lower() != name.lower():
                        content += f"| [{name}]({url}) | {desc} |\n"
                    else:
                        content += f"| [{name}]({url}) |  |\n"

        # ç»Ÿè®¡ä¿¡æ¯
        total_commits = len(weekly_data['commits'])
        total_links = sum(len(links) for links in unique_links.values())

        content += f"\n---\n\n"
        content += f"**ç»Ÿè®¡ï¼š** æœ¬å‘¨å…± {total_commits} æ¬¡æäº¤ï¼Œæ–°å¢ {total_links} ä¸ªé“¾æ¥ã€‚\n"

        return content

    def generate_weekly_files(self, start_date: str = "2025-07-21") -> List[str]:
        """ç”Ÿæˆæ‰€æœ‰å‘¨æŠ¥æ–‡ä»¶"""
        print("\n" + "="*60)
        print("ğŸ“… ç¬¬ä¸€æ­¥ï¼šä»Gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶")
        print("="*60)

        # è·å–æäº¤è®°å½•
        commits = self.get_git_log(start_date)

        if not commits:
            print("âš ï¸  æœªæ‰¾åˆ°æäº¤è®°å½•")
            return []

        # æŒ‰å‘¨åˆ†ç»„
        weeks = defaultdict(lambda: {'commits': [], 'links': defaultdict(list)})

        for commit in commits:
            monday, sunday = self.get_week_range(commit['date'])
            week_key = f"{monday}_{sunday}"

            weeks[week_key]['commits'].append(commit)

            # æå–è¯¥æäº¤çš„é“¾æ¥
            links_by_file = self.extract_links_from_diff(commit['hash'])
            for file, links in links_by_file.items():
                weeks[week_key]['links'][file].extend(links)

        # ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = []

        for week_key in sorted(weeks.keys()):
            week_data = weeks[week_key]
            monday, sunday = week_key.split('_')
            filename = f"weekly-{week_key}.md"
            filepath = self.weekly_dir / filename

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {filename}")
                generated_files.append(filename)
                continue

            # ç”Ÿæˆå†…å®¹
            content = f"# æœ¬å‘¨æ›´æ–° ({monday} ~ {sunday})\n\n"

            # å»é‡é“¾æ¥ï¼ˆä½¿ç”¨å­—å…¸ï¼Œurlä¸ºkeyï¼Œdescä¸ºvalueï¼‰
            unique_links = {}
            for file, links in week_data['links'].items():
                category = self.category_map.get(file, 'ğŸ“¦ å…¶ä»–')
                if category not in unique_links:
                    unique_links[category] = {}
                for link in links:
                    url = link['url']
                    desc = link['desc']
                    if url not in unique_links[category]:
                        unique_links[category][url] = desc
                    elif not unique_links[category][url] and desc:
                        unique_links[category][url] = desc

            # æŒ‰åˆ†ç±»è¾“å‡º
            for category in sorted(unique_links.keys()):
                links_dict = unique_links[category]
                if links_dict:
                    content += f"\n## {category}\n\n"
                    content += "| é¡¹ç›® | è¯´æ˜ |\n"
                    content += "|------|------|\n"

                    for url, desc in links_dict.items():
                        name = url.rstrip('/').split('/')[-1]
                        if desc and desc.lower() != name.lower():
                            content += f"| [{name}]({url}) | {desc} |\n"
                        else:
                            content += f"| [{name}]({url}) |  |\n"

            # ç»Ÿè®¡ä¿¡æ¯
            total_commits = len(week_data['commits'])
            total_links = sum(len(links) for links in unique_links.values())

            content += f"\n---\n\n"
            content += f"**ç»Ÿè®¡ï¼š** æœ¬å‘¨å…± {total_commits} æ¬¡æäº¤ï¼Œæ–°å¢ {total_links} ä¸ªé“¾æ¥ã€‚\n"

            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"âœ… ç”Ÿæˆ: {filename} ({total_links} ä¸ªé“¾æ¥)")
            generated_files.append(filename)

        print(f"\nğŸ“Š å…±ç”Ÿæˆ {len(generated_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶")
        return generated_files


class DescriptionGenerator:
    """æè¿°ç”Ÿæˆå™¨ - ä½¿ç”¨AIç”Ÿæˆé¡¹ç›®æè¿°"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / 'descriptions_cache.json'
        self.cache = self.load_cache()
        self.dirty = False  # æ ‡è®°ç¼“å­˜æ˜¯å¦è¢«ä¿®æ”¹

    def load_cache(self) -> Dict:
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_cache(self):
        """ä¿å­˜ç¼“å­˜ï¼ˆåªåœ¨æœ‰ä¿®æ”¹æ—¶æ‰å†™å…¥æ–‡ä»¶ï¼‰"""
        if not self.dirty:
            return  # æ²¡æœ‰ä¿®æ”¹ï¼Œè·³è¿‡å†™å…¥
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
        self.dirty = False

    def fetch_github_content(self, url: str) -> Optional[str]:
        """è·å–GitHubä»“åº“çš„READMEå†…å®¹ï¼ˆä½¿ç”¨raw.githubusercontent.comï¼Œæ— APIé™åˆ¶ï¼‰"""
        try:
            if 'github.com' not in url:
                return None

            parts = url.replace('https://github.com/', '').split('/')
            if len(parts) < 2:
                return None

            owner, repo = parts[0], parts[1]

            headers = {
                'User-Agent': 'Mozilla/5.0'
            }

            # 1. å°è¯•è·å–ä»“åº“ä¸»é¡µï¼ˆç”¨äºæå–æè¿°ï¼‰
            repo_page_url = f"https://github.com/{owner}/{repo}"
            try:
                page_response = requests.get(repo_page_url, headers=headers, timeout=5)
                description = ""
                if page_response.status_code == 200:
                    # ç®€å•æå–æè¿°ï¼ˆåœ¨<meta property="og:description"ä¸­ï¼‰
                    import re
                    desc_match = re.search(r'<meta property="og:description" content="([^"]*)"', page_response.text)
                    if desc_match:
                        description = desc_match.group(1)
            except:
                description = ""

            # 2. ç›´æ¥ä»raw.githubusercontent.comè·å–READMEï¼ˆæ— APIé™åˆ¶ï¼‰
            readme_content = ""

            # å°è¯•å¸¸è§çš„READMEæ–‡ä»¶å
            readme_files = ['README.md', 'readme.md', 'README.MD', 'README', 'README.txt']

            for readme_name in readme_files:
                raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{readme_name}"
                try:
                    readme_response = requests.get(raw_url, headers=headers, timeout=10)
                    if readme_response.status_code == 200:
                        readme_content = readme_response.text[:3000]
                        break
                except:
                    pass

                # å¦‚æœmainåˆ†æ”¯å¤±è´¥ï¼Œå°è¯•masteråˆ†æ”¯
                if not readme_content:
                    raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{readme_name}"
                    try:
                        readme_response = requests.get(raw_url, headers=headers, timeout=10)
                        if readme_response.status_code == 200:
                            readme_content = readme_response.text[:3000]
                            break
                    except:
                        pass

            # ç»„åˆå†…å®¹
            if readme_content or description:
                content = f"Repository: {owner}/{repo}\n"
                if description:
                    content += f"Description: {description}\n"
                if readme_content:
                    content += f"\nREADME (excerpt):\n{readme_content}"
                return content

            return None

        except Exception as e:
            return None

    def call_ai_for_summary(self, url: str, content: str) -> Optional[str]:
        """è°ƒç”¨AIæ¥å£ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹GitHubé¡¹ç›®ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æè¿°ï¼ˆ15-30ä¸ªæ±‰å­—ï¼‰ã€‚
è¦æ±‚ï¼š
1. çªå‡ºé¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½
2. ä½¿ç”¨ä¸“ä¸šæŠ€æœ¯æœ¯è¯­
3. ç®€æ´æ˜äº†ï¼Œä¾¿äºå¿«é€Ÿç†è§£

é¡¹ç›®é“¾æ¥: {url}

é¡¹ç›®ä¿¡æ¯:
{content}

è¯·åªè¿”å›ä¸­æ–‡æè¿°ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

            headers = {"Content-Type": "application/json"}

            # åˆ¤æ–­APIç±»å‹ï¼šæ£€æŸ¥URLæ˜¯å¦åŒ…å«/v1/messagesï¼ˆAnthropicæ ¼å¼ï¼‰
            is_anthropic_format = "/v1/messages" in AI_API_URL.lower() or "anthropic.com" in AI_API_URL.lower()

            # æ ¹æ®ä¸åŒçš„AIæ¥å£æ ¼å¼è°ƒæ•´è¯·æ±‚
            if is_anthropic_format:
                payload = {
                    "model": AI_MODEL,
                    "max_tokens": 100,
                    "messages": [{"role": "user", "content": prompt}]
                }
                headers["anthropic-version"] = "2023-06-01"

                # ä¼˜å…ˆä½¿ç”¨ AUTH_TOKEN (OAuth)ï¼Œå¦åˆ™ä½¿ç”¨ API_KEY
                if AI_AUTH_TOKEN:
                    headers["Authorization"] = f"Bearer {AI_AUTH_TOKEN}"
                elif AI_API_KEY:
                    headers["x-api-key"] = AI_API_KEY

            elif "ollama" in AI_API_URL.lower():
                payload = {
                    "model": AI_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
            else:
                payload = {
                    "model": AI_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": 100
                }
                if AI_API_KEY:
                    headers["Authorization"] = f"Bearer {AI_API_KEY}"

            response = requests.post(AI_API_URL, json=payload, headers=headers, timeout=30)

            if response.status_code == 200:
                result = response.json()

                if is_anthropic_format:
                    description = result.get('content', [{}])[0].get('text', '').strip()
                elif "ollama" in AI_API_URL.lower():
                    description = result.get('message', {}).get('content', '').strip()
                else:
                    description = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()

                description = description.strip('"\'').strip()
                return description
            else:
                print(f"    âœ— AI API é”™è¯¯: HTTP {response.status_code}")
                print(f"    âœ— å“åº”å†…å®¹: {response.text[:200]}")
                return None

        except Exception as e:
            print(f"    âœ— AI API å¼‚å¸¸: {e}")
            return None

    def is_cached(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²ç¼“å­˜"""
        return url in self.cache

    def get_cached(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æè¿°"""
        return self.cache.get(url)

    def generate_description(self, url: str) -> Optional[str]:
        """ç”Ÿæˆå•ä¸ªURLçš„æè¿°ï¼ˆå¸¦ç¼“å­˜ï¼‰

        è¿”å›å€¼:
        - å­—ç¬¦ä¸²: æ­£å¸¸æè¿°
        - "__DELETED__": é“¾æ¥404/ä¸å¯ç”¨ï¼Œåº”ä»å‘¨æŠ¥ä¸­åˆ é™¤
        - None: ç”Ÿæˆå¤±è´¥ä½†å¯ä»¥é‡è¯•
        """
        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            return self.cache[url]

        # è·å–å†…å®¹
        content = self.fetch_github_content(url)
        if not content:
            # ç¼“å­˜æ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œé¿å…é‡å¤è¯·æ±‚404é“¾æ¥
            self.cache[url] = "__DELETED__"
            self.dirty = True
            return "__DELETED__"

        # è°ƒç”¨AIç”Ÿæˆæè¿°
        description = self.call_ai_for_summary(url, content)

        if description and len(description) > 5:
            self.cache[url] = description
            self.dirty = True  # æ ‡è®°ç¼“å­˜å·²ä¿®æ”¹
            return description

        return None


class WeeklyUpdater:
    """å‘¨æŠ¥æ›´æ–°å™¨ - æ›´æ–°å‘¨æŠ¥æ–‡ä»¶ä¸­çš„æè¿°"""

    def __init__(self, weekly_dir: Path):
        self.weekly_dir = weekly_dir

    def extract_links_needing_descriptions(self, file_path: Path) -> List[str]:
        """æå–éœ€è¦æè¿°çš„é“¾æ¥"""
        links = []
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'
        matches = re.findall(pattern, content)

        for _, url, desc in matches:
            if not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc:
                links.append(url)

        return links

    def update_weekly_file(self, file_path: Path, descriptions: Dict[str, str]) -> Tuple[int, int]:
        """æ›´æ–°å‘¨æŠ¥æ–‡ä»¶

        è¿”å›: (æ›´æ–°æ•°é‡, åˆ é™¤æ•°é‡)
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        update_count = 0
        delete_count = 0
        new_lines = []
        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'

        for line in lines:
            match = re.search(pattern, line)
            if match:
                _, url, desc = match.groups()

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤ï¼ˆ404é“¾æ¥ï¼‰
                if url in descriptions and descriptions[url] == "__DELETED__":
                    delete_count += 1
                    continue  # è·³è¿‡æ­¤è¡Œï¼Œä¸å†™å…¥æ–°æ–‡ä»¶

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æè¿°
                if (not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc) and url in descriptions:
                    if descriptions[url] != "__DELETED__":
                        update_count += 1
                        name = url.split('/')[-1]
                        line = f'| [{name}]({url}) | {descriptions[url]} |\n'

            new_lines.append(line)

        if update_count > 0 or delete_count > 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)

        return update_count, delete_count


class AutoWeeklyProcessor:
    """å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥å¤„ç†å™¨"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.generator = WeeklyGenerator(repo_path)
        self.desc_gen = DescriptionGenerator(CACHE_DIR)
        self.updater = WeeklyUpdater(WEEKLY_DIR)

    def process_existing_weeklies(self, max_links_per_week: int = 50):
        """ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“ ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        print("="*60)
        print(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        weekly_files = sorted([
            f for f in os.listdir(str(WEEKLY_DIR))
            if f.startswith('weekly-') and f.endswith('.md')
        ])

        print(f"å‘ç° {len(weekly_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶\n")

        for i, filename in enumerate(weekly_files, 1):
            file_path = WEEKLY_DIR / filename
            print(f"\n{'='*60}")
            print(f"[{i}/{len(weekly_files)}] å¤„ç†: {filename}")
            print('='*60)

            links = self.updater.extract_links_needing_descriptions(file_path)
            if not links:
                print("  âœ“ å·²å®Œæˆ")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            if len(links) > max_links_per_week:
                print(f"âš ï¸  æœ¬æ¬¡åªå¤„ç†å‰ {max_links_per_week} ä¸ªé“¾æ¥")
                links = links[:max_links_per_week]

            descriptions = {}

            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] å¤„ç†: {url}")

                # å…ˆæ£€æŸ¥ç¼“å­˜
                if self.desc_gen.is_cached(url):
                    desc = self.desc_gen.get_cached(url)
                    if desc == "__DELETED__":
                        print(f"    âŠ˜ è·³è¿‡ (é“¾æ¥ä¸å¯ç”¨)")
                        descriptions[url] = desc  # ä¼ é€’åˆ é™¤æ ‡è®°
                    else:
                        print(f"    âœ“ ç¼“å­˜å‘½ä¸­: {desc}")
                        descriptions[url] = desc
                    continue

                # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè¯·æ±‚
                print(f"    â†’ è·å–GitHubå†…å®¹...")
                desc = self.desc_gen.generate_description(url)

                if desc == "__DELETED__":
                    print(f"    âŠ˜ é“¾æ¥ä¸å¯ç”¨ (404)")
                    descriptions[url] = desc  # ä¼ é€’åˆ é™¤æ ‡è®°
                elif desc:
                    print(f"    âœ“ ç”Ÿæˆ: {desc}")
                    descriptions[url] = desc

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        self.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                time.sleep(1)  # åªåœ¨ç½‘ç»œè¯·æ±‚åæ‰sleep

            self.desc_gen.save_cache()

            if descriptions:
                print(f"\nğŸ“ æ›´æ–°å‘¨æŠ¥æ–‡ä»¶...")
                updated, deleted = self.updater.update_weekly_file(file_path, descriptions)
                if updated > 0:
                    print(f"âœ… æˆåŠŸæ›´æ–° {updated} ä¸ªæè¿°åˆ° {filename}")
                if deleted > 0:
                    print(f"ğŸ—‘ï¸  åˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        print("="*60)

    def process_current_week(self, max_links: int = 50):
        """ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥ï¼ˆå«AIæè¿°ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“… ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥")
        print("="*60)

        # è·å–å½“å‰å‘¨çš„æ—¥æœŸèŒƒå›´
        today = datetime.now()
        # è®¡ç®—æœ¬å‘¨ä¸€ï¼ˆå‘¨ä¸€æ˜¯0ï¼‰
        days_since_monday = today.weekday()
        monday = today - timedelta(days=days_since_monday)
        sunday = monday + timedelta(days=6)

        week_start = monday.strftime('%Y-%m-%d')
        week_end = sunday.strftime('%Y-%m-%d')

        print(f"ğŸ“Š å½“å‰å‘¨æœŸ: {week_start} ~ {week_end}")
        print(f"ğŸ“Š æœ€å¤šå¤„ç†: {max_links} ä¸ªé“¾æ¥\n")

        # ç”Ÿæˆæœ¬å‘¨çš„å‘¨æŠ¥æ–‡ä»¶
        filename = f"weekly-{week_start}_{week_end}.md"
        file_path = WEEKLY_DIR / filename

        print(f"ğŸ“ ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶: {filename}")

        # è·å–æœ¬å‘¨çš„Gitæäº¤
        weekly_data = self.generator.get_weekly_commits(week_start, week_end)

        if not weekly_data['commits']:
            print(f"âš ï¸  æœ¬å‘¨ ({week_start} ~ {week_end}) æ²¡æœ‰æäº¤è®°å½•")
            return

        print(f"âœ“ å‘ç° {len(weekly_data['commits'])} ä¸ªæäº¤")

        # æå–æ‰€æœ‰é“¾æ¥
        links_by_file = self.generator.extract_links_from_diffs(weekly_data['commits'])

        # ç»Ÿè®¡æ€»é“¾æ¥æ•°
        total_links = sum(len(links) for links in links_by_file.values())
        print(f"âœ“ æå– {total_links} ä¸ªé“¾æ¥")

        if total_links == 0:
            print("âš ï¸  æœ¬å‘¨æ²¡æœ‰æ–°å¢é“¾æ¥")
            return

        # ç”Ÿæˆmarkdownå†…å®¹
        content = self.generator.generate_markdown(week_start, week_end, weekly_data, links_by_file)

        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ“ å‘¨æŠ¥æ–‡ä»¶å·²ç”Ÿæˆ: {file_path}")

        # ä¸ºé“¾æ¥ç”Ÿæˆæè¿°
        print(f"\n{'='*60}")
        print("ğŸ¤– å¼€å§‹ä¸ºé“¾æ¥ç”ŸæˆAIæè¿°...")
        print('='*60)

        links = self.updater.extract_links_needing_descriptions(file_path)

        if not links:
            print("âœ… æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°")
            return

        print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

        if len(links) > max_links:
            print(f"âš ï¸  é“¾æ¥è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {max_links} ä¸ª")
            links = links[:max_links]

        descriptions = {}

        for j, url in enumerate(links, 1):
            print(f"\n  [{j}/{len(links)}] {url}")

            # å…ˆæ£€æŸ¥ç¼“å­˜
            if self.desc_gen.is_cached(url):
                desc = self.desc_gen.get_cached(url)
                if desc == "__DELETED__":
                    print(f"    âŠ˜ è·³è¿‡ (é“¾æ¥ä¸å¯ç”¨)")
                    descriptions[url] = desc
                else:
                    print(f"    âœ“ ç¼“å­˜å‘½ä¸­: {desc}")
                    descriptions[url] = desc
                continue

            # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè¯·æ±‚
            print(f"    â†’ è·å–GitHubå†…å®¹...")
            desc = self.desc_gen.generate_description(url)

            if desc == "__DELETED__":
                print(f"    âŠ˜ é“¾æ¥ä¸å¯ç”¨ (404)")
                descriptions[url] = desc
            elif desc:
                print(f"    âœ“ ç”Ÿæˆ: {desc}")
                descriptions[url] = desc

                # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                if j % 5 == 0:
                    self.desc_gen.save_cache()
                    print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
            else:
                print(f"    âœ— ç”Ÿæˆå¤±è´¥")

            time.sleep(1)  # åªåœ¨ç½‘ç»œè¯·æ±‚åæ‰sleep

        # ä¿å­˜ç¼“å­˜
        self.desc_gen.save_cache()

        # æ›´æ–°æ–‡ä»¶
        if descriptions:
            updated, deleted = self.updater.update_weekly_file(file_path, descriptions)
            if updated > 0:
                print(f"\nâœ… æˆåŠŸæ›´æ–° {updated} ä¸ªæè¿°")
            if deleted > 0:
                print(f"ğŸ—‘ï¸  åˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")

        print("\n" + "="*60)
        print(f"ğŸ‰ å½“å‰å‘¨å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶ä½ç½®: {file_path}")
        print("="*60)

    def process_all(self, start_date: str = "2025-07-21", max_links_per_week: int = 50):
        """å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸš€ å¯åŠ¨å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆæµç¨‹")
        print("="*60)
        print(f"ğŸ“ ä»“åº“è·¯å¾„: {self.repo_path}")
        print(f"ğŸ¤– AIæ¨¡å‹: {AI_MODEL}")
        print(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        # æ­¥éª¤1: ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = self.generator.generate_weekly_files(start_date)

        if not generated_files:
            print("\nâš ï¸  æ²¡æœ‰å¯å¤„ç†çš„å‘¨æŠ¥æ–‡ä»¶")
            return

        # æ­¥éª¤2: ä¸ºæ¯ä¸ªå‘¨æŠ¥ç”Ÿæˆæè¿°
        print("\n" + "="*60)
        print("ğŸ“ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆé¡¹ç›®æè¿°å¹¶æ›´æ–°å‘¨æŠ¥")
        print("="*60)

        for i, filename in enumerate(generated_files, 1):
            file_path = WEEKLY_DIR / filename

            print(f"\n{'#'*60}")
            print(f"# [{i}/{len(generated_files)}] å¤„ç†: {filename}")
            print(f"{'#'*60}")

            # æå–éœ€è¦æè¿°çš„é“¾æ¥
            links = self.updater.extract_links_needing_descriptions(file_path)

            if not links:
                print("âœ… æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            # é™åˆ¶å¤„ç†æ•°é‡
            if len(links) > max_links_per_week:
                print(f"âš ï¸  é“¾æ¥è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {max_links_per_week} ä¸ª")
                links = links[:max_links_per_week]

            descriptions = {}

            # å¤„ç†æ¯ä¸ªé“¾æ¥
            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] {url}")

                # å…ˆæ£€æŸ¥ç¼“å­˜
                if self.desc_gen.is_cached(url):
                    desc = self.desc_gen.get_cached(url)
                    print(f"    âœ“ ç¼“å­˜å‘½ä¸­: {desc}")
                    descriptions[url] = desc
                    continue

                # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè¯·æ±‚
                print(f"    â†’ è·å–GitHubå†…å®¹...")
                description = self.desc_gen.generate_description(url)

                if description:
                    print(f"    âœ“ ç”Ÿæˆ: {description}")
                    descriptions[url] = description

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        self.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                # é¿å…è¯·æ±‚è¿‡å¿«ï¼ˆåªåœ¨ç½‘ç»œè¯·æ±‚åæ‰sleepï¼‰
                time.sleep(1)

            # ä¿å­˜ç¼“å­˜
            self.desc_gen.save_cache()

            # æ›´æ–°æ–‡ä»¶
            if descriptions:
                updated, deleted = self.updater.update_weekly_file(file_path, descriptions)
                if updated > 0:
                    print(f"\nâœ… æˆåŠŸæ›´æ–° {updated} ä¸ªæè¿°")
                if deleted > 0:
                    print(f"ğŸ—‘ï¸  åˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        print("="*60)


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}åˆ†{secs:.1f}ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours}å°æ—¶{minutes}åˆ†{secs:.1f}ç§’"


def main():
    # è®°å½•å¼€å§‹æ—¶é—´
    script_start_time = time.time()

    # è®¾ç½®æ§åˆ¶å°ç¼–ç 
    import sys
    if sys.platform == 'win32':
        try:
            import codecs
            sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
        except:
            pass

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆå·¥å…·                              â•‘
â•‘   Gitå†å² â†’ å‘¨æŠ¥ç”Ÿæˆ â†’ AIæè¿° â†’ è‡ªåŠ¨æ›´æ–°                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # æ£€æŸ¥è®¤è¯é…ç½®ï¼ˆæ”¯æŒä¸¤ç§æ–¹å¼ï¼‰
    if not AI_API_KEY and not AI_AUTH_TOKEN:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½®è®¤è¯ä¿¡æ¯")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ä»»ä¸€ç¯å¢ƒå˜é‡ï¼š")
        print("  æ–¹å¼1 - API Key:")
        print("    Windows: $env:ANTHROPIC_API_KEY='your-key'")
        print("    Linux:   export ANTHROPIC_API_KEY='your-key'")
        print("\n  æ–¹å¼2 - OAuth Token:")
        print("    Windows: $env:ANTHROPIC_AUTH_TOKEN='your-token'")
        print("    Linux:   export ANTHROPIC_AUTH_TOKEN='your-token'")
        return

    # æ˜¾ç¤ºå½“å‰è®¤è¯æ–¹å¼
    if AI_AUTH_TOKEN:
        print(f"ğŸ” è®¤è¯æ–¹å¼: OAuth Token (ANTHROPIC_AUTH_TOKEN)")
    else:
        print(f"ğŸ” è®¤è¯æ–¹å¼: API Key (ANTHROPIC_API_KEY)")

    # é€‰æ‹©æ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("1. å®Œå…¨è‡ªåŠ¨åŒ–ï¼ˆç”Ÿæˆå‘¨æŠ¥ + AIæè¿°ï¼‰")
    print("2. ä»…ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆä¸ç”Ÿæˆæè¿°ï¼‰")
    print("3. ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°")
    print("4. ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥ï¼ˆå«AIæè¿°ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4): ").strip()

    processor = AutoWeeklyProcessor(GIT_REPO_PATH)

    if choice == "1":
        # å®Œå…¨è‡ªåŠ¨åŒ–
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        max_links = int(input("æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: 50): ").strip() or "50")
        processor.process_all(start_date, max_links)

    elif choice == "2":
        # ä»…ç”Ÿæˆå‘¨æŠ¥
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        processor.generator.generate_weekly_files(start_date)

    elif choice == "3":
        # ä»…ç”Ÿæˆæè¿°
        print("\næ­¤æ¨¡å¼å°†ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        max_links = int(input("æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: 50): ").strip() or "50")

        weekly_files = sorted([
            f for f in os.listdir(WEEKLY_DIR)
            if f.startswith('weekly-') and f.endswith('.md')
        ])

        for i, filename in enumerate(weekly_files, 1):
            file_path = WEEKLY_DIR / filename
            print(f"\n{'='*60}")
            print(f"[{i}/{len(weekly_files)}] å¤„ç†: {filename}")
            print('='*60)

            links = processor.updater.extract_links_needing_descriptions(file_path)
            if not links:
                print("âœ“ è¯¥æ–‡ä»¶æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°\n")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            if len(links) > max_links:
                print(f"âš ï¸  æœ¬æ¬¡åªå¤„ç†å‰ {max_links} ä¸ªé“¾æ¥")
                links = links[:max_links]

            descriptions = {}

            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] å¤„ç†: {url}")

                # å…ˆæ£€æŸ¥ç¼“å­˜
                if processor.desc_gen.is_cached(url):
                    desc = processor.desc_gen.get_cached(url)
                    print(f"    âœ“ ç¼“å­˜å‘½ä¸­: {desc}")
                    descriptions[url] = desc
                    continue

                # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè¯·æ±‚
                print(f"    â†’ è·å–GitHubå†…å®¹...")
                desc = processor.desc_gen.generate_description(url)

                if desc:
                    print(f"    âœ“ ç”Ÿæˆ: {desc}")
                    descriptions[url] = desc

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        processor.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                time.sleep(1)  # åªåœ¨ç½‘ç»œè¯·æ±‚åæ‰sleep

            processor.desc_gen.save_cache()

            if descriptions:
                print(f"\nğŸ“ æ›´æ–°å‘¨æŠ¥æ–‡ä»¶...")
                updated, deleted = processor.updater.update_weekly_file(file_path, descriptions)
                if updated > 0:
                    print(f"âœ… æˆåŠŸæ›´æ–° {updated} ä¸ªæè¿°")
                if deleted > 0:
                    print(f"ğŸ—‘ï¸  åˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
                print()
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°\n")

    elif choice == "4":
        # ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥
        max_links = int(input("æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: 50): ").strip() or "50")
        processor.process_current_week(max_links)

    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")

    # æ‰“å°æ€»è¿è¡Œæ—¶é—´
    total_time = time.time() - script_start_time
    print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´: {format_duration(total_time)}")


if __name__ == "__main__":
    main()
