#!/usr/bin/env python3
"""
å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥ç”Ÿæˆå·¥å…·
1. ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆåŸºäºgen_weekly.pyï¼‰
2. è‡ªåŠ¨è·å–GitHubå†…å®¹
3. ä½¿ç”¨AIç”Ÿæˆä¸­æ–‡æè¿°
4. æ›´æ–°å‘¨æŠ¥æ–‡ä»¶
"""
import re
import os
import sys
import json
import time
import logging
import subprocess
import requests
import functools
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from json import JSONDecodeError
from typing import Dict, List, Optional, Tuple, Callable, Any
from urllib.parse import urlparse
from collections import defaultdict

# ============ æ—¥å¿—é…ç½® ============
def setup_logging(log_file: Optional[str] = None, level: int = logging.INFO):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    handlers = []

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_format = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_format)
    handlers.append(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_format)
        handlers.append(file_handler)

    logging.basicConfig(level=level, handlers=handlers)
    return logging.getLogger(__name__)

logger = setup_logging()

# ============ é‡è¯•è£…é¥°å™¨ ============
def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0,
          exceptions: tuple = (requests.RequestException,)):
    """
    é‡è¯•è£…é¥°å™¨

    Args:
        max_attempts: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
        backoff: å»¶è¿Ÿå€æ•°ï¼ˆæ¯æ¬¡é‡è¯•åå»¶è¿Ÿä¹˜ä»¥æ­¤å€¼ï¼‰
        exceptions: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            current_delay = delay
            last_exception = None

            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts:
                        logger.warning(f"    âš ï¸ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
                        logger.info(f"    â³ {current_delay:.1f}ç§’åé‡è¯•...")
                        time.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(f"    âœ— å·²é‡è¯• {max_attempts} æ¬¡ï¼Œæ”¾å¼ƒ")

            raise last_exception
        return wrapper
    return decorator

# ============ é…ç½®ç®¡ç† ============
@dataclass
class Config:
    """é…ç½®ç±»"""
    # AIæ¥å£é…ç½®
    ai_base_url: str = "https://api.anthropic.com"
    ai_api_key: str = ""
    ai_auth_token: str = ""
    ai_model: str = "claude-sonnet-4-5-20250929"

    # è·¯å¾„é…ç½®
    repo_path: Path = field(default_factory=Path.cwd)
    weekly_dir: Path = field(default=None)
    cache_dir: Path = field(default=None)

    # å¤„ç†é…ç½®
    max_links_per_week: int = 50
    request_delay: float = 1.0
    cache_save_interval: int = 5

    # é‡è¯•é…ç½®
    retry_max_attempts: int = 3
    retry_delay: float = 1.0
    retry_backoff: float = 2.0

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.weekly_dir is None:
            self.weekly_dir = self.repo_path / "weekly"
        if self.cache_dir is None:
            self.cache_dir = self.repo_path / "links_cache"

    @property
    def ai_api_url(self) -> str:
        """è·å–å®Œæ•´çš„AI API URL"""
        base = self.ai_base_url.rstrip("/")
        if base.endswith("/messages"):
            return base
        elif base.endswith("/v1"):
            return base + "/messages"
        else:
            return base + "/v1/messages"

    @classmethod
    def from_env(cls, repo_path: Optional[Path] = None) -> "Config":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        if repo_path is None:
            repo_path = Path.cwd()

        return cls(
            ai_base_url=os.getenv("ANTHROPIC_BASE_URL", "https://api.anthropic.com"),
            ai_api_key=os.getenv("ANTHROPIC_API_KEY", ""),
            ai_auth_token=os.getenv("ANTHROPIC_AUTH_TOKEN", ""),
            ai_model=os.getenv("AI_MODEL", "claude-sonnet-4-5-20250929"),
            repo_path=repo_path,
            max_links_per_week=int(os.getenv("MAX_LINKS_PER_WEEK", "50")),
            request_delay=float(os.getenv("REQUEST_DELAY", "1.0")),
        )

    @classmethod
    def from_file(cls, config_file: Path, repo_path: Optional[Path] = None) -> "Config":
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        if repo_path is None:
            repo_path = Path.cwd()

        config_data = {}
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    if config_file.suffix in ('.yaml', '.yml'):
                        try:
                            import yaml
                            config_data = yaml.safe_load(f) or {}
                        except ImportError:
                            logger.warning("æœªå®‰è£… PyYAMLï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®")
                    elif config_file.suffix == '.json':
                        config_data = json.load(f)
            except Exception as e:
                logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

        # åˆå¹¶ç¯å¢ƒå˜é‡ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆçº§æ›´é«˜ï¼‰
        return cls(
            ai_base_url=os.getenv("ANTHROPIC_BASE_URL", config_data.get("ai_base_url", "https://api.anthropic.com")),
            ai_api_key=os.getenv("ANTHROPIC_API_KEY", config_data.get("ai_api_key", "")),
            ai_auth_token=os.getenv("ANTHROPIC_AUTH_TOKEN", config_data.get("ai_auth_token", "")),
            ai_model=os.getenv("AI_MODEL", config_data.get("ai_model", "claude-sonnet-4-5-20250929")),
            repo_path=repo_path,
            max_links_per_week=int(os.getenv("MAX_LINKS_PER_WEEK", config_data.get("max_links_per_week", 50))),
            request_delay=float(os.getenv("REQUEST_DELAY", config_data.get("request_delay", 1.0))),
            retry_max_attempts=config_data.get("retry_max_attempts", 3),
            retry_delay=config_data.get("retry_delay", 1.0),
            retry_backoff=config_data.get("retry_backoff", 2.0),
        )

# å…¨å±€é…ç½®ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_config: Optional[Config] = None

def get_config() -> Config:
    """è·å–å…¨å±€é…ç½®"""
    global _config
    if _config is None:
        repo_path = Path.cwd()
        config_file = repo_path / "config.yaml"
        if config_file.exists():
            _config = Config.from_file(config_file, repo_path)
        else:
            _config = Config.from_env(repo_path)
    return _config

def set_config(config: Config):
    """è®¾ç½®å…¨å±€é…ç½®"""
    global _config
    _config = config

# ============ è¿›åº¦æ˜¾ç¤º ============
class ProgressBar:
    """ç®€å•çš„è¿›åº¦æ¡æ˜¾ç¤º"""

    def __init__(self, total: int, desc: str = "", width: int = 40):
        self.total = total
        self.current = 0
        self.desc = desc
        self.width = width
        self.start_time = time.time()

    def update(self, n: int = 1):
        """æ›´æ–°è¿›åº¦"""
        self.current += n
        self._display()

    def set(self, n: int):
        """è®¾ç½®å½“å‰è¿›åº¦"""
        self.current = n
        self._display()

    def _display(self):
        """æ˜¾ç¤ºè¿›åº¦æ¡"""
        if self.total == 0:
            return

        percent = self.current / self.total
        filled = int(self.width * percent)
        bar = 'â–ˆ' * filled + 'â–‘' * (self.width - filled)

        elapsed = time.time() - self.start_time
        if self.current > 0:
            eta = elapsed / self.current * (self.total - self.current)
            eta_str = format_duration(eta)
        else:
            eta_str = "--"

        # ä½¿ç”¨ \r å›åˆ°è¡Œé¦–ï¼Œè¦†ç›–ä¹‹å‰çš„è¾“å‡º
        sys.stdout.write(f'\r{self.desc} |{bar}| {self.current}/{self.total} ({percent:.0%}) ETA: {eta_str}')
        sys.stdout.flush()

    def finish(self):
        """å®Œæˆè¿›åº¦æ¡"""
        self.current = self.total
        self._display()
        print()  # æ¢è¡Œ


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}åˆ†{secs:.0f}ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}å°æ—¶{minutes}åˆ†"
# ================================


def _run_git(repo_path: Path, args: List[str], *, check: bool = True) -> subprocess.CompletedProcess:
    return subprocess.run(
        ['git', '-C', str(repo_path), *args],
        capture_output=True,
        text=True,
        encoding='utf-8',
        errors='ignore',
        check=check,
    )


class WeeklyGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨ - ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.weekly_dir = self.repo_path / "weekly"
        self.weekly_dir.mkdir(exist_ok=True)

        # æ–‡ä»¶ç±»å‹åˆ°åˆ†ç±»çš„æ˜ å°„
        self.category_map = {
            'README.md': 'ğŸ“¦ æ”¶é›†çš„é¡¹ç›®',
            'tools.md': 'ğŸ”§ æ”¶é›†çš„å·¥å…·',
            'BOF.md': 'ğŸ¯ BOFå·¥å…·',
            'skills-ai.md': 'ğŸ¤– AIä½¿ç”¨æŠ€å·§',
            'docs.md': 'ğŸ“š æ”¶é›†çš„æ–‡ç« ',
            'free.md': 'ğŸ å…è´¹èµ„æº',
            'pico.md': 'ğŸ”Œ PICOå·¥å…·'
        }

    def get_week_range(self, date_str: str) -> Tuple[str, str]:
        """è·å–æ—¥æœŸæ‰€åœ¨å‘¨çš„å‘¨ä¸€åˆ°å‘¨æ—¥"""
        date = datetime.strptime(date_str, "%Y-%m-%d")
        monday = date - timedelta(days=date.weekday())
        sunday = monday + timedelta(days=6)
        return monday.strftime("%Y-%m-%d"), sunday.strftime("%Y-%m-%d")

    def get_git_log(self, since_date: str = None) -> List[Dict]:
        """è·å–gitæäº¤æ—¥å¿—"""
        args = ['log', '--pretty=format:%H|%ad|%s', '--date=format:%Y-%m-%d']
        if since_date:
            args.extend(['--since', since_date])

        try:
            result = _run_git(self.repo_path, args, check=True)
        except subprocess.CalledProcessError as e:
            print(f"? git log ??: {e}")
            if e.stderr:
                print(f"   stderr: {e.stderr.strip()[:200]}")
            return []

        commits = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|', 2)
            if len(parts) == 3:
                commits.append({
                    'hash': parts[0],
                    'date': parts[1],
                    'message': parts[2]
                })

        return commits

    def extract_links_from_diff(self, commit_hash: str) -> Dict[str, List[Dict]]:
        """ä»æäº¤diffä¸­æå–é“¾æ¥ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç±»ï¼ˆæ’é™¤weeklyç›®å½•ï¼‰

        å¤„ç†é€»è¾‘ï¼š
        - GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
        - éGitHubé“¾æ¥ï¼šä½¿ç”¨æ ‡é¢˜è¡Œä½œä¸ºdesc
        """
        try:
            result = _run_git(self.repo_path, ['show', commit_hash, '--format=', '--unified=0'], check=True)
        except subprocess.CalledProcessError as e:
            print(f"? git show ??: {commit_hash}: {e}")
            if e.stderr:
                print(f"   stderr: {e.stderr.strip()[:200]}")
            return {}

        links_by_file = defaultdict(list)
        current_file = None
        last_title = ""  # ä¿å­˜ä¸Šä¸€è¡Œçš„æ ‡é¢˜ï¼ˆç”¨äºéGitHubé“¾æ¥ï¼‰

        for line in result.stdout.split('\n'):
            # æ£€æµ‹æ–‡ä»¶å
            if line.startswith('diff --git'):
                match = re.search(r'b/(.+)$', line)
                if match:
                    current_file = match.group(1)
                    last_title = ""
                continue

            # åªå¤„ç†æ–°å¢çš„è¡Œ
            if not line.startswith('+') or line.startswith('+++'):
                continue

            # å»æ‰å¼€å¤´çš„+å·
            content = line[1:]

            # æ’é™¤weeklyç›®å½•ä¸‹çš„æ–‡ä»¶
            if not current_file or current_file not in self.category_map:
                continue
            if current_file.startswith('weekly/'):
                continue

            # æ£€æµ‹æ ‡é¢˜è¡Œ (#### æ ‡é¢˜ æˆ– ### æ ‡é¢˜ ç­‰)
            title_match = re.match(r'^#{1,6}\s+(.+)$', content)
            if title_match:
                last_title = title_match.group(1).strip()
                continue

            # 1. åŒ¹é…markdowné“¾æ¥æ ¼å¼: [text](https://...)
            markdown_pattern = r'\[([^\]]+)\]\((https?://[^\)]+)\)'
            markdown_matches = re.findall(markdown_pattern, content)
            for text, url in markdown_matches:
                if 'github.com' in url:
                    # GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
                    links_by_file[current_file].append({'url': url, 'desc': ''})
                else:
                    # éGitHubé“¾æ¥ï¼šä½¿ç”¨markdownä¸­çš„textä½œä¸ºdesc
                    links_by_file[current_file].append({'url': url, 'desc': text})

            # 2. åŒ¹é…çº¯URLæ ¼å¼: https://...
            markdown_urls = [m[1] for m in markdown_matches]
            url_pattern = r'https?://[^\s\)\]>]+'
            url_matches = re.findall(url_pattern, content)
            for url in url_matches:
                if url not in markdown_urls:
                    if 'github.com' in url:
                        # GitHubé“¾æ¥ï¼šdescç•™ç©ºï¼Œåç»­ç”¨AIç”Ÿæˆ
                        links_by_file[current_file].append({'url': url, 'desc': ''})
                    else:
                        # éGitHubé“¾æ¥ï¼šä½¿ç”¨ä¸Šä¸€è¡Œçš„æ ‡é¢˜ä½œä¸ºdesc
                        links_by_file[current_file].append({'url': url, 'desc': last_title})
                    last_title = ""  # ä½¿ç”¨åæ¸…ç©º

            # å¦‚æœè¿™è¡Œä¸æ˜¯æ ‡é¢˜ä¹Ÿä¸æ˜¯é“¾æ¥ï¼Œæ¸…ç©ºlast_title
            if not title_match and not url_matches and not markdown_matches:
                if content.strip():  # éç©ºè¡Œ
                    last_title = ""

        return dict(links_by_file)

    def get_weekly_commits(self, week_start: str, week_end: str) -> Dict:
        """è·å–æŒ‡å®šå‘¨çš„æäº¤è®°å½•"""
        args = [
            'log',
            '--pretty=format:%H|%ad|%s',
            '--date=format:%Y-%m-%d',
            f'--since={week_start}',
            f'--until={week_end} 23:59:59',
        ]

        try:
            result = _run_git(self.repo_path, args, check=True)
        except subprocess.CalledProcessError as e:
            print(f"? git log ??: {e}")
            if e.stderr:
                print(f"   stderr: {e.stderr.strip()[:200]}")
            return {'commits': []}

        commits = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|', 2)
            if len(parts) == 3:
                commits.append({
                    'hash': parts[0],
                    'date': parts[1],
                    'message': parts[2]
                })

        return {'commits': commits}

    def extract_links_from_diffs(self, commits: List[Dict]) -> Dict[str, List[Dict]]:
        """ä»å¤šä¸ªæäº¤çš„diffä¸­æå–é“¾æ¥ï¼ˆæ’é™¤weeklyç›®å½•ï¼‰"""
        all_links = defaultdict(list)

        for commit in commits:
            links_by_file = self.extract_links_from_diff(commit['hash'])
            for file, links in links_by_file.items():
                all_links[file].extend(links)

        return dict(all_links)

    def generate_markdown(self, week_start: str, week_end: str, weekly_data: Dict, links_by_file: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆå‘¨æŠ¥çš„markdownå†…å®¹"""
        content = f"# æœ¬å‘¨æ›´æ–° ({week_start} ~ {week_end})\n\n"

        # å»é‡å¹¶æŒ‰åˆ†ç±»ç»„ç»‡é“¾æ¥
        unique_links = {}
        for file, links in links_by_file.items():
            category = self.category_map.get(file, 'ğŸ“¦ å…¶ä»–')
            if category not in unique_links:
                unique_links[category] = {}
            # ä½¿ç”¨urlä½œä¸ºkeyå»é‡ï¼Œä¿ç•™desc
            for link in links:
                url = link['url']
                desc = link['desc']
                if url not in unique_links[category]:
                    unique_links[category][url] = desc
                elif not unique_links[category][url] and desc:
                    # å¦‚æœå·²æœ‰urlä½†æ²¡æœ‰descï¼Œæ›´æ–°desc
                    unique_links[category][url] = desc

        # æŒ‰åˆ†ç±»è¾“å‡º
        for category in sorted(unique_links.keys()):
            links_dict = unique_links[category]
            if links_dict:
                content += f"\n## {category}\n\n"
                content += "| é¡¹ç›® | è¯´æ˜ |\n"
                content += "|------|------|\n"

                for url, desc in links_dict.items():
                    # ä»URLæå–é¡¹ç›®å
                    name = url.rstrip('/').split('/')[-1]
                    # å¦‚æœdescæ˜¯é¡¹ç›®åæœ¬èº«ï¼Œæ¸…ç©ºå®ƒè®©AIç”Ÿæˆ
                    if desc and desc.lower() != name.lower():
                        content += f"| [{name}]({url}) | {desc} |\n"
                    else:
                        content += f"| [{name}]({url}) |  |\n"

        # ç»Ÿè®¡ä¿¡æ¯
        total_commits = len(weekly_data['commits'])
        total_links = sum(len(links) for links in unique_links.values())

        content += f"\n---\n\n"
        content += f"**ç»Ÿè®¡ï¼š** æœ¬å‘¨å…± {total_commits} æ¬¡æäº¤ï¼Œæ–°å¢ {total_links} ä¸ªé“¾æ¥ã€‚\n"

        return content

    def generate_weekly_files(self, start_date: str = "2025-07-21") -> List[str]:
        """ç”Ÿæˆæ‰€æœ‰å‘¨æŠ¥æ–‡ä»¶"""
        print("\n" + "="*60)
        print("ğŸ“… ç¬¬ä¸€æ­¥ï¼šä»Gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶")
        print("="*60)

        # è·å–æäº¤è®°å½•
        commits = self.get_git_log(start_date)

        if not commits:
            print("âš ï¸  æœªæ‰¾åˆ°æäº¤è®°å½•")
            return []

        # æŒ‰å‘¨åˆ†ç»„
        weeks = defaultdict(lambda: {'commits': [], 'links': defaultdict(list)})

        for commit in commits:
            monday, sunday = self.get_week_range(commit['date'])
            week_key = f"{monday}_{sunday}"

            weeks[week_key]['commits'].append(commit)

            # æå–è¯¥æäº¤çš„é“¾æ¥
            links_by_file = self.extract_links_from_diff(commit['hash'])
            for file, links in links_by_file.items():
                weeks[week_key]['links'][file].extend(links)

        # ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = []

        for week_key in sorted(weeks.keys()):
            week_data = weeks[week_key]
            monday, sunday = week_key.split('_')
            filename = f"weekly-{week_key}.md"
            filepath = self.weekly_dir / filename

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {filename}")
                generated_files.append(filename)
                continue

            # ç”Ÿæˆå†…å®¹
            content = f"# æœ¬å‘¨æ›´æ–° ({monday} ~ {sunday})\n\n"

            # å»é‡é“¾æ¥ï¼ˆä½¿ç”¨å­—å…¸ï¼Œurlä¸ºkeyï¼Œdescä¸ºvalueï¼‰
            unique_links = {}
            for file, links in week_data['links'].items():
                category = self.category_map.get(file, 'ğŸ“¦ å…¶ä»–')
                if category not in unique_links:
                    unique_links[category] = {}
                for link in links:
                    url = link['url']
                    desc = link['desc']
                    if url not in unique_links[category]:
                        unique_links[category][url] = desc
                    elif not unique_links[category][url] and desc:
                        unique_links[category][url] = desc

            # æŒ‰åˆ†ç±»è¾“å‡º
            for category in sorted(unique_links.keys()):
                links_dict = unique_links[category]
                if links_dict:
                    content += f"\n## {category}\n\n"
                    content += "| é¡¹ç›® | è¯´æ˜ |\n"
                    content += "|------|------|\n"

                    for url, desc in links_dict.items():
                        name = url.rstrip('/').split('/')[-1]
                        if desc and desc.lower() != name.lower():
                            content += f"| [{name}]({url}) | {desc} |\n"
                        else:
                            content += f"| [{name}]({url}) |  |\n"

            # ç»Ÿè®¡ä¿¡æ¯
            total_commits = len(week_data['commits'])
            total_links = sum(len(links) for links in unique_links.values())

            content += f"\n---\n\n"
            content += f"**ç»Ÿè®¡ï¼š** æœ¬å‘¨å…± {total_commits} æ¬¡æäº¤ï¼Œæ–°å¢ {total_links} ä¸ªé“¾æ¥ã€‚\n"

            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"âœ… ç”Ÿæˆ: {filename} ({total_links} ä¸ªé“¾æ¥)")
            generated_files.append(filename)

        print(f"\nğŸ“Š å…±ç”Ÿæˆ {len(generated_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶")
        return generated_files


class DescriptionGenerator:
    """æè¿°ç”Ÿæˆå™¨ - ä½¿ç”¨AIç”Ÿæˆé¡¹ç›®æè¿°"""

    @dataclass(frozen=True)
    class GithubFetchResult:
        status: str  # ok | not_found | error | invalid
        content: Optional[str] = None
        http_status: Optional[int] = None
        error: Optional[str] = None

    @dataclass(frozen=True)
    class WebFetchResult:
        status: str  # ok | not_found | error
        content: Optional[str] = None
        http_status: Optional[int] = None
        error: Optional[str] = None

    def __init__(self, cache_dir: Path, config: Optional[Config] = None):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / 'descriptions_cache.json'
        self.cache = self.load_cache()
        self.dirty = False  # æ ‡è®°ç¼“å­˜æ˜¯å¦è¢«ä¿®æ”¹
        self.config = config or get_config()

    def load_cache(self) -> Dict:
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except JSONDecodeError:
                backup = self.cache_file.with_suffix(f".corrupt.{int(time.time())}.json")
                try:
                    self.cache_file.rename(backup)
                except OSError:
                    pass
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ JSON è§£æå¤±è´¥ï¼Œå·²å¤‡ä»½: {backup.name}")
        return {}

    def save_cache(self):
        """ä¿å­˜ç¼“å­˜ï¼ˆåªåœ¨æœ‰ä¿®æ”¹æ—¶æ‰å†™å…¥æ–‡ä»¶ï¼‰"""
        if not self.dirty:
            return  # æ²¡æœ‰ä¿®æ”¹ï¼Œè·³è¿‡å†™å…¥
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
        self.dirty = False

    @staticmethod
    def _parse_github_repo(url: str) -> Optional[Tuple[str, str]]:
        try:
            parsed = urlparse(url)
        except Exception:
            return None

        if parsed.scheme.lower() != 'https':
            return None

        netloc = (parsed.netloc or '').lower()
        if netloc != 'github.com':
            return None

        parts = [p for p in (parsed.path or '').split('/') if p]
        if len(parts) < 2:
            return None

        owner = parts[0].strip()
        repo = parts[1].strip()
        if repo.endswith('.git'):
            repo = repo[:-4]

        if not owner or not repo:
            return None

        return owner, repo

    def is_github_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºGitHub URL"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower() == 'github.com'
        except Exception:
            return False

    def _fetch_with_retry(self, url: str, headers: dict, timeout: int = 10) -> requests.Response:
        """å¸¦é‡è¯•çš„HTTPè¯·æ±‚"""
        config = self.config
        current_delay = config.retry_delay

        for attempt in range(1, config.retry_max_attempts + 1):
            try:
                response = requests.get(url, headers=headers, timeout=timeout)
                return response
            except requests.RequestException as e:
                if attempt < config.retry_max_attempts:
                    logger.warning(f"    âš ï¸ è¯·æ±‚å¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")
                    logger.info(f"    â³ {current_delay:.1f}ç§’åé‡è¯•...")
                    time.sleep(current_delay)
                    current_delay *= config.retry_backoff
                else:
                    raise

    def fetch_github_content(self, url: str) -> "DescriptionGenerator.GithubFetchResult":
        """è·å–GitHubä»“åº“çš„READMEå†…å®¹ï¼ˆä½¿ç”¨raw.githubusercontent.comï¼Œæ— APIé™åˆ¶ï¼‰"""
        repo_info = self._parse_github_repo(url)
        if not repo_info:
            return self.GithubFetchResult(status="invalid")

        owner, repo = repo_info

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        repo_page_url = f"https://github.com/{owner}/{repo}"
        try:
            page_response = self._fetch_with_retry(repo_page_url, headers, timeout=10)
        except requests.RequestException as e:
            return self.GithubFetchResult(status="error", error=str(e))

        if page_response.status_code == 404:
            return self.GithubFetchResult(status="not_found", http_status=404)

        description = ""
        if page_response.status_code == 200:
            desc_match = re.search(
                r'<meta\s+property="og:description"\s+content="([^"]*)"\s*/?>',
                page_response.text,
                flags=re.IGNORECASE,
            )
            if desc_match:
                description = desc_match.group(1)

        readme_content = ""
        readme_files = ['README.md', 'readme.md', 'README.MD', 'README', 'README.txt']
        for branch in ("main", "master"):
            for readme_name in readme_files:
                raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{readme_name}"
                try:
                    readme_response = self._fetch_with_retry(raw_url, headers, timeout=15)
                except requests.RequestException:
                    continue

                if readme_response.status_code == 200:
                    readme_content = readme_response.text[:3000]
                    break
            if readme_content:
                break

        if readme_content or description:
            content = f"Repository: {owner}/{repo}\n"
            if description:
                content += f"Description: {description}\n"
            if readme_content:
                content += f"\nREADME (excerpt):\n{readme_content}"
            return self.GithubFetchResult(status="ok", content=content, http_status=page_response.status_code)

        if page_response.status_code >= 400:
            return self.GithubFetchResult(
                status="error",
                http_status=page_response.status_code,
                error=(page_response.text or "")[:200],
            )

        return self.GithubFetchResult(status="error", http_status=page_response.status_code, error="No description/README found")

    def fetch_web_content(self, url: str) -> "DescriptionGenerator.WebFetchResult":
        """è·å–éGitHubç½‘é¡µå†…å®¹"""
        fetcher = WebContentFetcher(self.config)
        result = fetcher.fetch_web_content(url)
        return self.WebFetchResult(
            status=result.status,
            content=result.content,
            http_status=result.http_status,
            error=result.error
        )

    def _call_ai_with_retry(self, payload: dict, headers: dict) -> requests.Response:
        """å¸¦é‡è¯•çš„AI APIè¯·æ±‚"""
        config = self.config
        current_delay = config.retry_delay

        for attempt in range(1, config.retry_max_attempts + 1):
            try:
                response = requests.post(
                    config.ai_api_url,
                    json=payload,
                    headers=headers,
                    timeout=30
                )
                # å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œä¹Ÿéœ€è¦é‡è¯•
                if response.status_code == 429:
                    raise requests.RequestException(f"Rate limited: {response.status_code}")
                return response
            except requests.RequestException as e:
                if attempt < config.retry_max_attempts:
                    logger.warning(f"    âš ï¸ AIè¯·æ±‚å¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")
                    logger.info(f"    â³ {current_delay:.1f}ç§’åé‡è¯•...")
                    time.sleep(current_delay)
                    current_delay *= config.retry_backoff
                else:
                    raise

    def call_ai_for_summary(self, url: str, content: str) -> Optional[str]:
        """è°ƒç”¨AIæ¥å£ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        config = self.config

        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹GitHubé¡¹ç›®ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æè¿°ï¼ˆ15-30ä¸ªæ±‰å­—ï¼‰ã€‚
è¦æ±‚ï¼š
1. çªå‡ºé¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½
2. ä½¿ç”¨ä¸“ä¸šæŠ€æœ¯æœ¯è¯­
3. ç®€æ´æ˜äº†ï¼Œä¾¿äºå¿«é€Ÿç†è§£

é¡¹ç›®é“¾æ¥: {url}

é¡¹ç›®ä¿¡æ¯:
{content}

è¯·åªè¿”å›ä¸­æ–‡æè¿°ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

            headers = {"Content-Type": "application/json"}

            parsed = urlparse(config.ai_api_url)
            path = (parsed.path or "").lower().rstrip("/")
            is_anthropic_format = path.endswith("/v1/messages") or path.endswith("/messages")

            # æ ¹æ®ä¸åŒçš„AIæ¥å£æ ¼å¼è°ƒæ•´è¯·æ±‚
            if is_anthropic_format:
                payload = {
                    "model": config.ai_model,
                    "max_tokens": 100,
                    "messages": [{"role": "user", "content": prompt}]
                }
                headers["anthropic-version"] = "2023-06-01"

                # ä¼˜å…ˆä½¿ç”¨ AUTH_TOKEN (OAuth)ï¼Œå¦åˆ™ä½¿ç”¨ API_KEY
                if config.ai_auth_token:
                    headers["Authorization"] = f"Bearer {config.ai_auth_token}"
                elif config.ai_api_key:
                    headers["x-api-key"] = config.ai_api_key

            elif "ollama" in config.ai_api_url.lower():
                payload = {
                    "model": config.ai_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
            else:
                payload = {
                    "model": config.ai_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": 100
                }
                if config.ai_auth_token:
                    headers["Authorization"] = f"Bearer {config.ai_auth_token}"
                elif config.ai_api_key:
                    headers["Authorization"] = f"Bearer {config.ai_api_key}"

            response = self._call_ai_with_retry(payload, headers)

            if response.status_code == 200:
                result = response.json()

                if is_anthropic_format:
                    description = result.get('content', [{}])[0].get('text', '').strip()
                elif "ollama" in config.ai_api_url.lower():
                    description = result.get('message', {}).get('content', '').strip()
                else:
                    description = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()

                description = description.strip('"\'').strip()
                return description
            else:
                logger.error(f"    âœ— AI API é”™è¯¯: HTTP {response.status_code}")
                logger.debug(f"    âœ— å“åº”å†…å®¹: {response.text[:200]}")
                return None

        except Exception as e:
            logger.error(f"    âœ— AI API å¼‚å¸¸: {e}")
            return None

    def is_cached(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²ç¼“å­˜"""
        return url in self.cache

    def get_cached(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æè¿°"""
        return self.cache.get(url)

    def generate_description(self, url: str) -> Optional[str]:
        """ç”Ÿæˆå•ä¸ªURLçš„æè¿°ï¼ˆå¸¦ç¼“å­˜ï¼‰- æ”¯æŒGitHubå’Œæ™®é€šç½‘é¡µ

        è¿”å›å€¼:
        - å­—ç¬¦ä¸²: æ­£å¸¸æè¿°
        - "__DELETED__": é“¾æ¥404/ä¸å¯ç”¨ï¼Œåº”ä»æ–‡ä»¶ä¸­åˆ é™¤
        - None: ç”Ÿæˆå¤±è´¥ä½†å¯ä»¥é‡è¯•
        """
        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            return self.cache[url]

        # æ ¹æ®URLç±»å‹é€‰æ‹©æŠ“å–æ–¹å¼
        if self.is_github_url(url):
            fetch_result = self.fetch_github_content(url)
        else:
            fetch_result = self.fetch_web_content(url)

        if fetch_result.status == "not_found":
            self.cache[url] = "__DELETED__"
            self.dirty = True
            return "__DELETED__"

        if fetch_result.status != "ok" or not fetch_result.content:
            return None

        # è°ƒç”¨AIç”Ÿæˆæè¿°
        description = self.call_ai_for_summary(url, fetch_result.content)

        if description and len(description) > 5:
            self.cache[url] = description
            self.dirty = True  # æ ‡è®°ç¼“å­˜å·²ä¿®æ”¹
            return description

        return None


class WebContentFetcher:
    """ç½‘é¡µå†…å®¹æŠ“å–å™¨ - æ™ºèƒ½æå–éGitHubç½‘é¡µçš„æ­£æ–‡å†…å®¹"""

    @dataclass(frozen=True)
    class FetchResult:
        status: str  # ok | not_found | error
        content: Optional[str] = None
        http_status: Optional[int] = None
        error: Optional[str] = None

    def __init__(self, config: Optional[Config] = None):
        self.config = config or get_config()
        # éœ€è¦ç§»é™¤çš„æ ‡ç­¾
        self._remove_tags = {'script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript', 'iframe'}
        # æ­£æ–‡å®¹å™¨ä¼˜å…ˆçº§
        self._content_selectors = ['article', 'main', '.post-content', '.article-content', '.entry-content', '#content']

    def _extract_title(self, html: str) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        import re
        # å°è¯• <title>
        match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        # å°è¯• <h1>
        match = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return ""

    def _extract_meta_description(self, html: str) -> str:
        """æå– meta description"""
        import re
        match = re.search(
            r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']+)["\']',
            html, re.IGNORECASE
        )
        if match:
            return match.group(1).strip()
        # å°è¯•å¦ä¸€ç§é¡ºåº
        match = re.search(
            r'<meta\s+content=["\']([^"\']+)["\']\s+name=["\']description["\']',
            html, re.IGNORECASE
        )
        if match:
            return match.group(1).strip()
        return ""

    def _extract_og_description(self, html: str) -> str:
        """æå– Open Graph description"""
        import re
        match = re.search(
            r'<meta\s+property=["\']og:description["\']\s+content=["\']([^"\']+)["\']',
            html, re.IGNORECASE
        )
        if match:
            return match.group(1).strip()
        match = re.search(
            r'<meta\s+content=["\']([^"\']+)["\']\s+property=["\']og:description["\']',
            html, re.IGNORECASE
        )
        if match:
            return match.group(1).strip()
        return ""

    def _remove_unwanted_tags(self, html: str) -> str:
        """ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾åŠå…¶å†…å®¹"""
        import re
        for tag in self._remove_tags:
            html = re.sub(
                rf'<{tag}[^>]*>.*?</{tag}>',
                '', html, flags=re.IGNORECASE | re.DOTALL
            )
            # è‡ªé—­åˆæ ‡ç­¾
            html = re.sub(rf'<{tag}[^>]*/>', '', html, flags=re.IGNORECASE)
        return html

    def _extract_text_from_html(self, html: str) -> str:
        """ä»HTMLä¸­æå–çº¯æ–‡æœ¬"""
        import re
        # ç§»é™¤æ‰€æœ‰æ ‡ç­¾
        text = re.sub(r'<[^>]+>', ' ', html)
        # å¤„ç†HTMLå®ä½“
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&amp;', '&')
        text = text.replace('&quot;', '"')
        # åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _extract_main_content(self, html: str) -> str:
        """æ™ºèƒ½æå–æ­£æ–‡å†…å®¹"""
        import re

        # å…ˆç§»é™¤å¹²æ‰°æ ‡ç­¾
        cleaned_html = self._remove_unwanted_tags(html)

        # å°è¯•æ‰¾åˆ°æ­£æ–‡å®¹å™¨
        content = ""

        # 1. å°è¯• <article>
        match = re.search(r'<article[^>]*>(.*?)</article>', cleaned_html, re.IGNORECASE | re.DOTALL)
        if match:
            content = match.group(1)

        # 2. å°è¯• <main>
        if not content:
            match = re.search(r'<main[^>]*>(.*?)</main>', cleaned_html, re.IGNORECASE | re.DOTALL)
            if match:
                content = match.group(1)

        # 3. å°è¯•å¸¦æœ‰ content/post/article ç±»åçš„ div
        if not content:
            match = re.search(
                r'<div[^>]*class=["\'][^"\']*(?:content|post|article|entry)[^"\']*["\'][^>]*>(.*?)</div>',
                cleaned_html, re.IGNORECASE | re.DOTALL
            )
            if match:
                content = match.group(1)

        # 4. å›é€€åˆ° body
        if not content:
            match = re.search(r'<body[^>]*>(.*?)</body>', cleaned_html, re.IGNORECASE | re.DOTALL)
            if match:
                content = match.group(1)
            else:
                content = cleaned_html

        # æå–çº¯æ–‡æœ¬
        text = self._extract_text_from_html(content)
        return text

    def fetch_web_content(self, url: str) -> "WebContentFetcher.FetchResult":
        """æŠ“å–ç½‘é¡µå†…å®¹å¹¶æ™ºèƒ½æå–æ­£æ–‡"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5,zh-CN;q=0.3',
        }

        try:
            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)

            if response.status_code == 404:
                return self.FetchResult(status="not_found", http_status=404)

            if response.status_code >= 400:
                return self.FetchResult(
                    status="error",
                    http_status=response.status_code,
                    error=f"HTTP {response.status_code}"
                )

            html = response.text

            # æå–å„éƒ¨åˆ†å†…å®¹
            title = self._extract_title(html)
            meta_desc = self._extract_meta_description(html)
            og_desc = self._extract_og_description(html)
            main_content = self._extract_main_content(html)

            # ç»„åˆå†…å®¹
            content_parts = []
            if title:
                content_parts.append(f"Title: {title}")
            if meta_desc:
                content_parts.append(f"Meta Description: {meta_desc}")
            elif og_desc:
                content_parts.append(f"Description: {og_desc}")
            if main_content:
                # é™åˆ¶æ­£æ–‡é•¿åº¦
                truncated = main_content[:3000]
                content_parts.append(f"\nContent:\n{truncated}")

            if not content_parts:
                return self.FetchResult(
                    status="error",
                    http_status=response.status_code,
                    error="No extractable content"
                )

            return self.FetchResult(
                status="ok",
                content="\n".join(content_parts),
                http_status=response.status_code
            )

        except requests.Timeout:
            return self.FetchResult(status="error", error="Request timeout")
        except requests.RequestException as e:
            return self.FetchResult(status="error", error=str(e))


class WeeklyUpdater:
    """å‘¨æŠ¥æ›´æ–°å™¨ - æ›´æ–°å‘¨æŠ¥æ–‡ä»¶ä¸­çš„æè¿°"""

    def __init__(self, weekly_dir: Path):
        self.weekly_dir = weekly_dir

    def extract_links_needing_descriptions(self, file_path: Path) -> List[str]:
        """æå–éœ€è¦æè¿°çš„é“¾æ¥"""
        links = []
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'
        matches = re.findall(pattern, content)

        for _, url, desc in matches:
            if not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc:
                links.append(url)

        return links

    def update_weekly_file(self, file_path: Path, descriptions: Dict[str, str]) -> Tuple[int, int]:
        """æ›´æ–°å‘¨æŠ¥æ–‡ä»¶

        è¿”å›: (æ›´æ–°æ•°é‡, åˆ é™¤æ•°é‡)
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        update_count = 0
        delete_count = 0
        new_lines = []
        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'

        for line in lines:
            match = re.search(pattern, line)
            if match:
                _, url, desc = match.groups()

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤ï¼ˆ404é“¾æ¥ï¼‰
                if url in descriptions and descriptions[url] == "__DELETED__":
                    delete_count += 1
                    continue  # è·³è¿‡æ­¤è¡Œï¼Œä¸å†™å…¥æ–°æ–‡ä»¶

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æè¿°
                if (not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc) and url in descriptions:
                    if descriptions[url] != "__DELETED__":
                        update_count += 1
                        name = url.split('/')[-1]
                        line = f'| [{name}]({url}) | {descriptions[url]} |\n'

            new_lines.append(line)

        if update_count > 0 or delete_count > 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)

        return update_count, delete_count


class SourceFileUpdater:
    """æºæ–‡ä»¶æ›´æ–°å™¨ - æ›´æ–° docs.md/README.md ç­‰æºæ–‡ä»¶ä¸ºè¡¨æ ¼æ ¼å¼"""

    # éœ€è¦å¤„ç†çš„æºæ–‡ä»¶åˆ—è¡¨
    SOURCE_FILES = ['docs.md', 'README.md', 'tools.md', 'BOF.md', 'skills-ai.md', 'pico.md']

    def __init__(self):
        pass

    def extract_urls(self, file_path: Path) -> List[str]:
        """ä»æ–‡ä»¶ä¸­æå–æ‰€æœ‰URL"""
        urls = []
        content = file_path.read_text(encoding='utf-8')

        # åŒ¹é…ç‹¬ç«‹è¡Œçš„URL
        url_pattern = r'^(https?://[^\s]+)$'
        for match in re.finditer(url_pattern, content, re.MULTILINE):
            urls.append(match.group(1).strip())

        # åŒ¹é…markdowné“¾æ¥æ ¼å¼
        md_pattern = r'\[([^\]]+)\]\((https?://[^\)]+)\)'
        for match in re.finditer(md_pattern, content):
            url = match.group(2).strip()
            if url not in urls:
                urls.append(url)

        return urls

    def _extract_title_from_context(self, content: str, url: str) -> str:
        """ä»URLä¸Šä¸‹æ–‡æå–æ ‡é¢˜"""
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if url in line:
                # å‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„æ ‡é¢˜è¡Œ
                for j in range(i - 1, max(0, i - 5), -1):
                    title_match = re.match(r'^#{1,6}\s+(.+)$', lines[j].strip())
                    if title_match:
                        return title_match.group(1).strip()
                break
        # ä»URLæå–åç§°ä½œä¸ºå›é€€
        return url.rstrip('/').split('/')[-1]

    def _group_urls_by_section(self, file_path: Path) -> Dict[str, List[Tuple[str, str, str]]]:
        """æŒ‰ç« èŠ‚åˆ†ç»„URLï¼Œè¿”å› {section_title: [(url, original_title, existing_desc), ...]}

        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. åŸå§‹æ ¼å¼: #### æ ‡é¢˜ + URLå•ç‹¬ä¸€è¡Œ
        2. è¡¨æ ¼æ ¼å¼: | [æ ‡é¢˜](URL) | æè¿° |
        """
        content = file_path.read_text(encoding='utf-8')
        lines = content.split('\n')

        sections = {}
        current_section = "é»˜è®¤"
        current_title = ""

        for line in lines:
            stripped = line.strip()

            # æ£€æµ‹ä¸€çº§æˆ–äºŒçº§æ ‡é¢˜ä½œä¸ºç« èŠ‚
            section_match = re.match(r'^(#{1,2})\s+(.+)$', stripped)
            if section_match:
                current_section = section_match.group(2).strip()
                continue

            # æ£€æµ‹ä¸‰çº§åŠä»¥ä¸‹æ ‡é¢˜ä½œä¸ºæ¡ç›®æ ‡é¢˜
            title_match = re.match(r'^#{3,6}\s+(.+)$', stripped)
            if title_match:
                current_title = title_match.group(1).strip()
                continue

            # æ£€æµ‹è¡¨æ ¼è¡Œæ ¼å¼: | [æ ‡é¢˜](URL) | æè¿° |
            table_match = re.match(r'^\|\s*\[([^\]]+)\]\((https?://[^\)]+)\)\s*\|\s*(.*?)\s*\|$', stripped)
            if table_match:
                title = table_match.group(1).strip()
                url = table_match.group(2).strip()
                existing_desc = table_match.group(3).strip()
                if current_section not in sections:
                    sections[current_section] = []
                sections[current_section].append((url, title, existing_desc))
                continue

            # æ£€æµ‹URLï¼ˆåŸå§‹æ ¼å¼ï¼‰
            url_match = re.match(r'^(https?://[^\s]+)$', stripped)
            if url_match:
                url = url_match.group(1)
                if current_section not in sections:
                    sections[current_section] = []
                sections[current_section].append((url, current_title or url.split('/')[-1], ""))
                current_title = ""  # é‡ç½®æ ‡é¢˜

        return sections

    def update_source_file(self, file_path: Path, descriptions: Dict[str, str]) -> Tuple[int, int]:
        """æ›´æ–°æºæ–‡ä»¶ä¸ºè¡¨æ ¼æ ¼å¼

        è¿”å›: (æ›´æ–°æ•°é‡, åˆ é™¤æ•°é‡)
        """
        sections = self._group_urls_by_section(file_path)

        if not sections:
            return 0, 0

        # è¯»å–åŸå§‹æ–‡ä»¶è·å–ä¸»æ ‡é¢˜
        original_content = file_path.read_text(encoding='utf-8')
        main_title_match = re.match(r'^#\s+(.+)$', original_content, re.MULTILINE)
        main_title = main_title_match.group(1) if main_title_match else file_path.stem

        # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
        backup_path = file_path.with_suffix(file_path.suffix + '.bak')
        backup_path.write_text(original_content, encoding='utf-8')
        logger.info(f"  ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½: {backup_path.name}")

        # æ„å»ºæ–°å†…å®¹
        new_lines = [f"# {main_title}\n"]
        updated = 0
        deleted = 0

        for section, url_tuples in sections.items():
            # å…ˆç»Ÿè®¡æœ¬ç« èŠ‚æœ‰æ•ˆçš„URLæ•°é‡
            valid_rows = []
            section_deleted = 0

            for url, title, existing_desc in url_tuples:
                # ä¼˜å…ˆä½¿ç”¨æ–°ç”Ÿæˆçš„æè¿°ï¼Œå¦åˆ™ä¿ç•™å·²æœ‰æè¿°
                desc = descriptions.get(url, "")

                if desc == "__DELETED__":
                    section_deleted += 1
                    continue

                if desc:
                    updated += 1
                elif existing_desc:
                    # ä¿ç•™å·²æœ‰æè¿°ï¼ˆè¡¨æ ¼æ ¼å¼ä¸­å·²æœ‰çš„ï¼‰
                    desc = existing_desc
                else:
                    desc = ""  # ä¿ç•™ç©ºæè¿°

                # ä½¿ç”¨URLæœ€åä¸€æ®µä½œä¸ºæ˜¾ç¤ºåç§°
                display_name = title or url.rstrip('/').split('/')[-1]
                valid_rows.append(f"| [{display_name}]({url}) | {desc} |\n")

            deleted += section_deleted

            # è·³è¿‡ç©ºç« èŠ‚ï¼ˆæ‰€æœ‰URLéƒ½è¢«åˆ é™¤çš„æƒ…å†µï¼‰
            if not valid_rows:
                continue

            # æ·»åŠ ç« èŠ‚æ ‡é¢˜
            if section != main_title and section != "é»˜è®¤":
                new_lines.append(f"\n## {section}\n")

            # æ·»åŠ è¡¨æ ¼
            new_lines.append("\n| æ–‡ç«  | ç®€ä»‹ |")
            new_lines.append("\n|------|------|\n")
            new_lines.extend(valid_rows)

        # å†™å…¥æ–‡ä»¶
        file_path.write_text(''.join(new_lines), encoding='utf-8')

        return updated, deleted

    def get_source_files(self, repo_path: Path) -> List[Path]:
        """è·å–éœ€è¦å¤„ç†çš„æºæ–‡ä»¶åˆ—è¡¨"""
        files = []
        for filename in self.SOURCE_FILES:
            file_path = repo_path / filename
            if file_path.exists():
                files.append(file_path)
        return files


class AutoWeeklyProcessor:
    """å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥å¤„ç†å™¨"""

    def __init__(self, repo_path: Optional[str] = None, config: Optional[Config] = None):
        self.config = config or get_config()
        self.repo_path = Path(repo_path) if repo_path else self.config.repo_path
        self.generator = WeeklyGenerator(str(self.repo_path))
        self.desc_gen = DescriptionGenerator(self.config.cache_dir, self.config)
        self.updater = WeeklyUpdater(self.config.weekly_dir)
        self.source_updater = SourceFileUpdater()

    def _process_links(self, links: List[str], max_links: int, show_progress: bool = True) -> Dict[str, str]:
        """
        å¤„ç†é“¾æ¥åˆ—è¡¨ï¼Œç”Ÿæˆæè¿°ï¼ˆå…¬å…±æ–¹æ³•ï¼Œæ¶ˆé™¤ä»£ç é‡å¤ï¼‰

        Args:
            links: éœ€è¦å¤„ç†çš„é“¾æ¥åˆ—è¡¨
            max_links: æœ€å¤§å¤„ç†æ•°é‡
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

        Returns:
            Dict[str, str]: URL -> æè¿°çš„æ˜ å°„
        """
        if len(links) > max_links:
            logger.info(f"âš ï¸  é“¾æ¥è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {max_links} ä¸ª")
            links = links[:max_links]

        descriptions = {}
        config = self.config

        # åˆå§‹åŒ–è¿›åº¦æ¡
        progress = ProgressBar(len(links), desc="å¤„ç†é“¾æ¥") if show_progress else None

        for j, url in enumerate(links, 1):
            if not show_progress:
                logger.info(f"\n  [{j}/{len(links)}] {url}")

            # å…ˆæ£€æŸ¥ç¼“å­˜
            if self.desc_gen.is_cached(url):
                desc = self.desc_gen.get_cached(url)
                if desc == "__DELETED__":
                    if not show_progress:
                        logger.info(f"    âŠ˜ è·³è¿‡ (é“¾æ¥ä¸å¯ç”¨)")
                else:
                    if not show_progress:
                        logger.info(f"    âœ“ ç¼“å­˜å‘½ä¸­: {desc}")
                descriptions[url] = desc
                if progress:
                    progress.update()
                continue

            # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè¯·æ±‚
            if not show_progress:
                logger.info(f"    â†’ è·å–GitHubå†…å®¹...")

            desc = self.desc_gen.generate_description(url)

            if desc == "__DELETED__":
                if not show_progress:
                    logger.info(f"    âŠ˜ é“¾æ¥ä¸å¯ç”¨ (404)")
                descriptions[url] = desc
            elif desc:
                if not show_progress:
                    logger.info(f"    âœ“ ç”Ÿæˆ: {desc}")
                descriptions[url] = desc

                # å®šæœŸä¿å­˜ç¼“å­˜
                if j % config.cache_save_interval == 0:
                    self.desc_gen.save_cache()
                    if not show_progress:
                        logger.info(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
            else:
                if not show_progress:
                    logger.info(f"    âœ— ç”Ÿæˆå¤±è´¥")

            if progress:
                progress.update()

            # åªåœ¨ç½‘ç»œè¯·æ±‚åæ‰sleep
            time.sleep(config.request_delay)

        if progress:
            progress.finish()

        # æœ€ç»ˆä¿å­˜ç¼“å­˜
        self.desc_gen.save_cache()

        return descriptions

    def _process_weekly_file(self, file_path: Path, max_links: int, file_index: int = 0,
                              total_files: int = 0, show_progress: bool = True) -> Tuple[int, int]:
        """
        å¤„ç†å•ä¸ªå‘¨æŠ¥æ–‡ä»¶ï¼ˆå…¬å…±æ–¹æ³•ï¼Œæ¶ˆé™¤ä»£ç é‡å¤ï¼‰

        Args:
            file_path: å‘¨æŠ¥æ–‡ä»¶è·¯å¾„
            max_links: æœ€å¤§å¤„ç†é“¾æ¥æ•°
            file_index: å½“å‰æ–‡ä»¶ç´¢å¼•ï¼ˆç”¨äºæ˜¾ç¤ºè¿›åº¦ï¼‰
            total_files: æ€»æ–‡ä»¶æ•°ï¼ˆç”¨äºæ˜¾ç¤ºè¿›åº¦ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

        Returns:
            Tuple[int, int]: (æ›´æ–°æ•°é‡, åˆ é™¤æ•°é‡)
        """
        filename = file_path.name

        if file_index > 0 and total_files > 0:
            logger.info(f"\n{'='*60}")
            logger.info(f"[{file_index}/{total_files}] å¤„ç†: {filename}")
            logger.info('='*60)
        else:
            logger.info(f"\nå¤„ç†: {filename}")

        links = self.updater.extract_links_needing_descriptions(file_path)

        if not links:
            logger.info("  âœ“ æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°")
            return 0, 0

        logger.info(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

        # å¤„ç†é“¾æ¥
        descriptions = self._process_links(links, max_links, show_progress)

        # æ›´æ–°æ–‡ä»¶
        if descriptions:
            logger.info(f"\nğŸ“ æ›´æ–°å‘¨æŠ¥æ–‡ä»¶...")
            updated, deleted = self.updater.update_weekly_file(file_path, descriptions)
            if updated > 0:
                logger.info(f"âœ… æˆåŠŸæ›´æ–° {updated} ä¸ªæè¿°åˆ° {filename}")
            if deleted > 0:
                logger.info(f"ğŸ—‘ï¸  åˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
            return updated, deleted
        else:
            logger.info(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")
            return 0, 0

    def process_existing_weeklies(self, max_links_per_week: int = None):
        """ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰"""
        if max_links_per_week is None:
            max_links_per_week = self.config.max_links_per_week

        logger.info("\n" + "="*60)
        logger.info("ğŸ“ ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        logger.info("="*60)
        logger.info(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        weekly_dir = self.config.weekly_dir
        weekly_files = sorted([
            f for f in os.listdir(str(weekly_dir))
            if f.startswith('weekly-') and f.endswith('.md')
        ])

        logger.info(f"å‘ç° {len(weekly_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶\n")

        total_updated = 0
        total_deleted = 0

        for i, filename in enumerate(weekly_files, 1):
            file_path = weekly_dir / filename
            updated, deleted = self._process_weekly_file(
                file_path, max_links_per_week,
                file_index=i, total_files=len(weekly_files),
                show_progress=False
            )
            total_updated += updated
            total_deleted += deleted

        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è®¡æ›´æ–°: {total_updated} ä¸ªæè¿°ï¼Œåˆ é™¤: {total_deleted} ä¸ªæ— æ•ˆé“¾æ¥")
        logger.info("="*60)

    def process_current_week(self, max_links: int = None):
        """ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥ï¼ˆå«AIæè¿°ï¼‰"""
        if max_links is None:
            max_links = self.config.max_links_per_week

        logger.info("\n" + "="*60)
        logger.info("ğŸ“… ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥")
        logger.info("="*60)

        # è·å–å½“å‰å‘¨çš„æ—¥æœŸèŒƒå›´
        today = datetime.now()
        days_since_monday = today.weekday()
        monday = today - timedelta(days=days_since_monday)
        sunday = monday + timedelta(days=6)

        week_start = monday.strftime('%Y-%m-%d')
        week_end = sunday.strftime('%Y-%m-%d')

        logger.info(f"ğŸ“Š å½“å‰å‘¨æœŸ: {week_start} ~ {week_end}")
        logger.info(f"ğŸ“Š æœ€å¤šå¤„ç†: {max_links} ä¸ªé“¾æ¥\n")

        # ç”Ÿæˆæœ¬å‘¨çš„å‘¨æŠ¥æ–‡ä»¶
        weekly_dir = self.config.weekly_dir
        filename = f"weekly-{week_start}_{week_end}.md"
        file_path = weekly_dir / filename

        logger.info(f"ğŸ“ ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶: {filename}")

        # è·å–æœ¬å‘¨çš„Gitæäº¤
        weekly_data = self.generator.get_weekly_commits(week_start, week_end)

        if not weekly_data['commits']:
            logger.warning(f"âš ï¸  æœ¬å‘¨ ({week_start} ~ {week_end}) æ²¡æœ‰æäº¤è®°å½•")
            return

        logger.info(f"âœ“ å‘ç° {len(weekly_data['commits'])} ä¸ªæäº¤")

        # æå–æ‰€æœ‰é“¾æ¥
        links_by_file = self.generator.extract_links_from_diffs(weekly_data['commits'])
        total_links = sum(len(links) for links in links_by_file.values())
        logger.info(f"âœ“ æå– {total_links} ä¸ªé“¾æ¥")

        if total_links == 0:
            logger.warning("âš ï¸  æœ¬å‘¨æ²¡æœ‰æ–°å¢é“¾æ¥")
            return

        # ç”Ÿæˆmarkdownå†…å®¹
        content = self.generator.generate_markdown(week_start, week_end, weekly_data, links_by_file)

        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"âœ“ å‘¨æŠ¥æ–‡ä»¶å·²ç”Ÿæˆ: {file_path}")

        # ä¸ºé“¾æ¥ç”Ÿæˆæè¿°
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ¤– å¼€å§‹ä¸ºé“¾æ¥ç”ŸæˆAIæè¿°...")
        logger.info('='*60)

        updated, deleted = self._process_weekly_file(file_path, max_links, show_progress=False)

        logger.info("\n" + "="*60)
        logger.info(f"ğŸ‰ å½“å‰å‘¨å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼")
        logger.info(f"ğŸ“„ æ–‡ä»¶ä½ç½®: {file_path}")
        logger.info("="*60)

    def process_all(self, start_date: str = "2025-07-21", max_links_per_week: int = None):
        """å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†"""
        if max_links_per_week is None:
            max_links_per_week = self.config.max_links_per_week

        config = self.config

        logger.info("\n" + "="*60)
        logger.info("ğŸš€ å¯åŠ¨å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆæµç¨‹")
        logger.info("="*60)
        logger.info(f"ğŸ“ ä»“åº“è·¯å¾„: {self.repo_path}")
        logger.info(f"ğŸ¤– AIæ¨¡å‹: {config.ai_model}")
        logger.info(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        # æ­¥éª¤1: ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = self.generator.generate_weekly_files(start_date)

        if not generated_files:
            logger.warning("\nâš ï¸  æ²¡æœ‰å¯å¤„ç†çš„å‘¨æŠ¥æ–‡ä»¶")
            return

        # æ­¥éª¤2: ä¸ºæ¯ä¸ªå‘¨æŠ¥ç”Ÿæˆæè¿°
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆé¡¹ç›®æè¿°å¹¶æ›´æ–°å‘¨æŠ¥")
        logger.info("="*60)

        weekly_dir = self.config.weekly_dir
        total_updated = 0
        total_deleted = 0

        for i, filename in enumerate(generated_files, 1):
            file_path = weekly_dir / filename

            logger.info(f"\n{'#'*60}")
            logger.info(f"# [{i}/{len(generated_files)}] å¤„ç†: {filename}")
            logger.info(f"{'#'*60}")

            updated, deleted = self._process_weekly_file(
                file_path, max_links_per_week,
                show_progress=False
            )
            total_updated += updated
            total_deleted += deleted

        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è®¡æ›´æ–°: {total_updated} ä¸ªæè¿°ï¼Œåˆ é™¤: {total_deleted} ä¸ªæ— æ•ˆé“¾æ¥")
        logger.info("="*60)

    def process_source_files(self, max_links: int = None):
        """å¤„ç†æºæ–‡ä»¶ï¼ˆdocs.md, README.md ç­‰ï¼‰- ç”Ÿæˆæè¿°å¹¶è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼"""
        if max_links is None:
            max_links = self.config.max_links_per_week

        logger.info("\n" + "="*60)
        logger.info("ğŸ“„ å¤„ç†æºæ–‡ä»¶ï¼ˆç”Ÿæˆæè¿°å¹¶è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼ï¼‰")
        logger.info("="*60)

        # è·å–æºæ–‡ä»¶åˆ—è¡¨
        source_files = self.source_updater.get_source_files(self.repo_path)

        if not source_files:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æºæ–‡ä»¶")
            return

        logger.info(f"ğŸ“Š å‘ç° {len(source_files)} ä¸ªæºæ–‡ä»¶")
        for f in source_files:
            logger.info(f"   - {f.name}")
        logger.info("")

        total_updated = 0
        total_deleted = 0

        for i, file_path in enumerate(source_files, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"[{i}/{len(source_files)}] å¤„ç†: {file_path.name}")
            logger.info('='*60)

            # æå–URL
            urls = self.source_updater.extract_urls(file_path)

            if not urls:
                logger.info("  âœ“ æ²¡æœ‰éœ€è¦å¤„ç†çš„URL")
                continue

            logger.info(f"ğŸ“Š å‘ç° {len(urls)} ä¸ªURL")

            # é™åˆ¶å¤„ç†æ•°é‡
            if len(urls) > max_links:
                logger.info(f"âš ï¸  é“¾æ¥è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {max_links} ä¸ª")
                urls = urls[:max_links]

            # ç”Ÿæˆæè¿°
            descriptions = {}
            progress = ProgressBar(len(urls), desc="å¤„ç†URL")

            for url in urls:
                # æ£€æŸ¥ç¼“å­˜
                if self.desc_gen.is_cached(url):
                    desc = self.desc_gen.get_cached(url)
                else:
                    desc = self.desc_gen.generate_description(url)
                    time.sleep(self.config.request_delay)

                if desc:
                    descriptions[url] = desc

                progress.update()

            progress.finish()

            # ä¿å­˜ç¼“å­˜
            self.desc_gen.save_cache()

            # æ›´æ–°æºæ–‡ä»¶
            if descriptions:
                logger.info(f"\nğŸ“ æ›´æ–°æºæ–‡ä»¶ä¸ºè¡¨æ ¼æ ¼å¼...")
                updated, deleted = self.source_updater.update_source_file(file_path, descriptions)
                logger.info(f"âœ… æ›´æ–° {updated} ä¸ªæè¿°ï¼Œåˆ é™¤ {deleted} ä¸ªæ— æ•ˆé“¾æ¥")
                total_updated += updated
                total_deleted += deleted

        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æºæ–‡ä»¶å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è®¡æ›´æ–°: {total_updated} ä¸ªæè¿°ï¼Œåˆ é™¤: {total_deleted} ä¸ªæ— æ•ˆé“¾æ¥")
        logger.info("="*60)

    def commit_changes(self):
        """æäº¤å‘¨æŠ¥æ–‡ä»¶å’Œç¼“å­˜çš„å˜æ›´"""
        logger.info("\n" + "="*60)
        logger.info("æäº¤å‘¨æŠ¥å˜æ›´")
        logger.info("="*60)

        try:
            # æ£€æŸ¥å˜æ›´
            logger.info("1. æ£€æŸ¥ 'git status'...")
            status_result = subprocess.run(
                ['git', '-C', str(self.repo_path), 'status', '--short'],
                capture_output=True, text=True, encoding='utf-8', errors='ignore'
            )
            if not status_result.stdout.strip():
                logger.info("âœ… æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å˜æ›´ï¼Œæ— éœ€æäº¤ã€‚")
                return

            logger.info("æ£€æµ‹åˆ°ä»¥ä¸‹å˜æ›´:")
            logger.info(status_result.stdout)

            # æ·»åŠ å‘¨æŠ¥æ–‡ä»¶
            logger.info("2. æ·»åŠ å˜æ›´åˆ°æš‚å­˜åŒº...")
            subprocess.run(
                ['git', '-C', str(self.repo_path), 'add', 'weekly/*.md'],
                check=True
            )
            subprocess.run(
                ['git', '-C', str(self.repo_path), 'add', '-f', 'links_cache/descriptions_cache.json'],
                check=True
            )
            logger.info("  âœ“ 'weekly/' ç›®å½•ä¸‹çš„ .md æ–‡ä»¶")
            logger.info("  âœ“ 'links_cache/descriptions_cache.json'")

            # æäº¤å˜æ›´
            commit_message = f"docs: weekly update {datetime.now().strftime('%Y-%m-%d')}"
            logger.info(f"3. æäº¤å˜æ›´ï¼Œæäº¤ä¿¡æ¯: '{commit_message}'...")
            subprocess.run(
                ['git', '-C', str(self.repo_path), 'commit', '-m', commit_message],
                check=True,
                capture_output=True, text=True, encoding='utf-8', errors='ignore'
            )
            logger.info("âœ… å˜æ›´å·²æˆåŠŸæäº¤ï¼")

        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ æäº¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            logger.error(f"  â†’ Stderr: {e.stderr}")
        except Exception as e:
            logger.error(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # è®°å½•å¼€å§‹æ—¶é—´
    script_start_time = time.time()

    # è®¾ç½®æ§åˆ¶å°ç¼–ç  (Windows)
    if sys.platform == 'win32':
        try:
            import codecs
            sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
        except:
            pass

    logger.info("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆå·¥å…·                              â•‘
â•‘   Gitå†å² â†’ å‘¨æŠ¥ç”Ÿæˆ â†’ AIæè¿° â†’ è‡ªåŠ¨æ›´æ–°                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # åˆå§‹åŒ–é…ç½®
    config = get_config()

    # æ£€æŸ¥è®¤è¯é…ç½®ï¼ˆæ”¯æŒä¸¤ç§æ–¹å¼ï¼‰
    if not config.ai_api_key and not config.ai_auth_token:
        logger.error("âŒ é”™è¯¯ï¼šæœªè®¾ç½®è®¤è¯ä¿¡æ¯")
        logger.info("\nè¯·è®¾ç½®ä»¥ä¸‹ä»»ä¸€ç¯å¢ƒå˜é‡ï¼š")
        logger.info("  æ–¹å¼1 - API Key:")
        logger.info("    Windows: $env:ANTHROPIC_API_KEY='your-key'")
        logger.info("    Linux:   export ANTHROPIC_API_KEY='your-key'")
        logger.info("\n  æ–¹å¼2 - OAuth Token:")
        logger.info("    Windows: $env:ANTHROPIC_AUTH_TOKEN='your-token'")
        logger.info("    Linux:   export ANTHROPIC_AUTH_TOKEN='your-token'")
        return

    # æ˜¾ç¤ºå½“å‰é…ç½®
    if config.ai_auth_token:
        logger.info(f"ğŸ” è®¤è¯æ–¹å¼: OAuth Token (ANTHROPIC_AUTH_TOKEN)")
    else:
        logger.info(f"ğŸ” è®¤è¯æ–¹å¼: API Key (ANTHROPIC_API_KEY)")

    logger.info(f"ğŸ“ ä»“åº“è·¯å¾„: {config.repo_path}")
    logger.info(f"ğŸ¤– AIæ¨¡å‹: {config.ai_model}")

    # é€‰æ‹©æ¨¡å¼
    logger.info("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    logger.info("1. å®Œå…¨è‡ªåŠ¨åŒ–ï¼ˆç”Ÿæˆå‘¨æŠ¥ + AIæè¿°ï¼‰")
    logger.info("2. ä»…ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆä¸ç”Ÿæˆæè¿°ï¼‰")
    logger.info("3. ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°")
    logger.info("4. ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥ï¼ˆå«AIæè¿°ï¼‰")
    logger.info("5. æäº¤å‘¨æŠ¥å˜æ›´")
    logger.info("6. å¤„ç†æºæ–‡ä»¶ï¼ˆdocs.mdç­‰è½¬è¡¨æ ¼ + AIæè¿°ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4/5/6): ").strip()

    processor = AutoWeeklyProcessor(config=config)

    if choice == "1":
        # å®Œå…¨è‡ªåŠ¨åŒ–
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        max_links = int(input(f"æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: {config.max_links_per_week}): ").strip() or str(config.max_links_per_week))
        processor.process_all(start_date, max_links)

    elif choice == "2":
        # ä»…ç”Ÿæˆå‘¨æŠ¥
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        processor.generator.generate_weekly_files(start_date)

    elif choice == "3":
        # ä»…ç”Ÿæˆæè¿°ï¼ˆä½¿ç”¨é‡æ„åçš„æ–¹æ³•ï¼Œæ¶ˆé™¤ä»£ç é‡å¤ï¼‰
        logger.info("\næ­¤æ¨¡å¼å°†ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        max_links = int(input(f"æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: {config.max_links_per_week}): ").strip() or str(config.max_links_per_week))
        processor.process_existing_weeklies(max_links)

    elif choice == "4":
        # ç”Ÿæˆå½“å‰å‘¨çš„å‘¨æŠ¥
        max_links = int(input(f"æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: {config.max_links_per_week}): ").strip() or str(config.max_links_per_week))
        processor.process_current_week(max_links)

    elif choice == "5":
        # æäº¤å‘¨æŠ¥å˜æ›´
        processor.commit_changes()

    elif choice == "6":
        # å¤„ç†æºæ–‡ä»¶
        logger.info("\næ­¤æ¨¡å¼å°†å¤„ç† docs.mdã€README.md ç­‰æºæ–‡ä»¶")
        logger.info("  - æå–æ‰€æœ‰URLï¼ˆGitHub + æ™®é€šç½‘é¡µï¼‰")
        logger.info("  - ä½¿ç”¨AIç”Ÿæˆä¸­æ–‡ç®€ä»‹")
        logger.info("  - è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼")
        max_links = int(input(f"\næœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: {config.max_links_per_week}): ").strip() or str(config.max_links_per_week))
        processor.process_source_files(max_links)

    else:
        logger.error("âŒ æ— æ•ˆçš„é€‰é¡¹")

    # æ‰“å°æ€»è¿è¡Œæ—¶é—´
    total_time = time.time() - script_start_time
    logger.info(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´: {format_duration(total_time)}")


if __name__ == "__main__":
    main()
