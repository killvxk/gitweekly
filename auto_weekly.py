#!/usr/bin/env python3
"""
å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥ç”Ÿæˆå·¥å…·
1. ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆåŸºäºgen_weekly.pyï¼‰
2. è‡ªåŠ¨è·å–GitHubå†…å®¹
3. ä½¿ç”¨AIç”Ÿæˆä¸­æ–‡æè¿°
4. æ›´æ–°å‘¨æŠ¥æ–‡ä»¶
"""
import re
import os
import json
import time
import subprocess
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# ============ é…ç½®åŒºåŸŸ ============
# AIæ¥å£é…ç½® - é»˜è®¤ä½¿ç”¨Anthropic Claude API
AI_API_URL = os.getenv("AI_API_URL", "https://api.anthropic.com/v1/messages")
AI_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
AI_MODEL = os.getenv("AI_MODEL", "claude-sonnet-4-5")  # ä½¿ç”¨Claude Sonnet 4.5ï¼ˆæœ€æ–°æœ€å¼ºï¼‰

# Gitä»“åº“é…ç½®
GIT_REPO_PATH = "f:/gitweekly"
WEEKLY_DIR = Path(GIT_REPO_PATH) / "weekly"
CACHE_DIR = Path(GIT_REPO_PATH) / "links_cache"
# ================================


class WeeklyGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨ - ä»gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.weekly_dir = self.repo_path / "weekly"
        self.weekly_dir.mkdir(exist_ok=True)

        # æ–‡ä»¶ç±»å‹åˆ°åˆ†ç±»çš„æ˜ å°„
        self.category_map = {
            'README.md': 'ğŸ“¦ æ”¶é›†çš„é¡¹ç›®',
            'tools.md': 'ğŸ”§ æ”¶é›†çš„å·¥å…·',
            'BOF.md': 'ğŸ¯ BOFå·¥å…·',
            'skills-ai.md': 'ğŸ¤– AIä½¿ç”¨æŠ€å·§',
            'docs.md': 'ğŸ“š æ”¶é›†çš„æ–‡ç« ',
            'free.md': 'ğŸ å…è´¹èµ„æº'
        }

    def get_week_range(self, date_str: str) -> Tuple[str, str]:
        """è·å–æ—¥æœŸæ‰€åœ¨å‘¨çš„å‘¨ä¸€åˆ°å‘¨æ—¥"""
        date = datetime.strptime(date_str, "%Y-%m-%d")
        monday = date - timedelta(days=date.weekday())
        sunday = monday + timedelta(days=6)
        return monday.strftime("%Y-%m-%d"), sunday.strftime("%Y-%m-%d")

    def get_git_log(self, since_date: str = None) -> List[Dict]:
        """è·å–gitæäº¤æ—¥å¿—"""
        cmd = [
            'git', '-C', str(self.repo_path),
            'log', '--pretty=format:%H|%ad|%s',
            '--date=format:%Y-%m-%d'
        ]

        if since_date:
            cmd.extend(['--since', since_date])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )

        commits = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|', 2)
            if len(parts) == 3:
                commits.append({
                    'hash': parts[0],
                    'date': parts[1],
                    'message': parts[2]
                })

        return commits

    def extract_links_from_diff(self, commit_hash: str) -> Dict[str, List[str]]:
        """ä»æäº¤diffä¸­æå–é“¾æ¥ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç±»"""
        cmd = [
            'git', '-C', str(self.repo_path),
            'show', commit_hash, '--format=', '--unified=0'
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )

        links_by_file = defaultdict(list)
        current_file = None

        for line in result.stdout.split('\n'):
            # æ£€æµ‹æ–‡ä»¶å
            if line.startswith('diff --git'):
                match = re.search(r'b/(.+)$', line)
                if match:
                    current_file = match.group(1)

            # æå–æ–°å¢çš„é“¾æ¥
            if line.startswith('+') and not line.startswith('+++'):
                # åŒ¹é…markdowné“¾æ¥
                pattern = r'\[([^\]]+)\]\((https://github\.com/[^\)]+)\)'
                matches = re.findall(pattern, line)

                for text, url in matches:
                    if current_file and current_file in self.category_map:
                        links_by_file[current_file].append(url)

        return dict(links_by_file)

    def generate_weekly_files(self, start_date: str = "2025-07-21") -> List[str]:
        """ç”Ÿæˆæ‰€æœ‰å‘¨æŠ¥æ–‡ä»¶"""
        print("\n" + "="*60)
        print("ğŸ“… ç¬¬ä¸€æ­¥ï¼šä»Gitå†å²ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶")
        print("="*60)

        # è·å–æäº¤è®°å½•
        commits = self.get_git_log(start_date)

        if not commits:
            print("âš ï¸  æœªæ‰¾åˆ°æäº¤è®°å½•")
            return []

        # æŒ‰å‘¨åˆ†ç»„
        weeks = defaultdict(lambda: {'commits': [], 'links': defaultdict(list)})

        for commit in commits:
            monday, sunday = self.get_week_range(commit['date'])
            week_key = f"{monday}_{sunday}"

            weeks[week_key]['commits'].append(commit)

            # æå–è¯¥æäº¤çš„é“¾æ¥
            links_by_file = self.extract_links_from_diff(commit['hash'])
            for file, links in links_by_file.items():
                weeks[week_key]['links'][file].extend(links)

        # ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = []

        for week_key in sorted(weeks.keys()):
            week_data = weeks[week_key]
            monday, sunday = week_key.split('_')
            filename = f"weekly-{week_key}.md"
            filepath = self.weekly_dir / filename

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {filename}")
                generated_files.append(filename)
                continue

            # ç”Ÿæˆå†…å®¹
            content = f"# æœ¬å‘¨æ›´æ–° ({monday} ~ {sunday})\n\n"

            # å»é‡é“¾æ¥
            unique_links = {}
            for file, links in week_data['links'].items():
                category = self.category_map.get(file, 'ğŸ“¦ å…¶ä»–')
                if category not in unique_links:
                    unique_links[category] = []
                unique_links[category].extend(list(set(links)))

            # æŒ‰åˆ†ç±»è¾“å‡º
            for category in sorted(unique_links.keys()):
                links = list(set(unique_links[category]))
                if links:
                    content += f"\n## {category}\n\n"
                    content += "| é¡¹ç›® | è¯´æ˜ |\n"
                    content += "|------|------|\n"

                    for url in links:
                        name = url.split('/')[-1]
                        content += f"| [{name}]({url}) |  |\n"

            # ç»Ÿè®¡ä¿¡æ¯
            total_commits = len(week_data['commits'])
            total_links = sum(len(links) for links in unique_links.values())

            content += f"\n---\n\n"
            content += f"**ç»Ÿè®¡ï¼š** æœ¬å‘¨å…± {total_commits} æ¬¡æäº¤ï¼Œæ–°å¢ {total_links} ä¸ªé“¾æ¥ã€‚\n"

            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"âœ… ç”Ÿæˆ: {filename} ({total_links} ä¸ªé“¾æ¥)")
            generated_files.append(filename)

        print(f"\nğŸ“Š å…±ç”Ÿæˆ {len(generated_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶")
        return generated_files


class DescriptionGenerator:
    """æè¿°ç”Ÿæˆå™¨ - ä½¿ç”¨AIç”Ÿæˆé¡¹ç›®æè¿°"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / 'descriptions_cache.json'
        self.cache = self.load_cache()

    def load_cache(self) -> Dict:
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_cache(self):
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def fetch_github_content(self, url: str) -> Optional[str]:
        """è·å–GitHubä»“åº“çš„READMEå†…å®¹ï¼ˆä½¿ç”¨raw.githubusercontent.comï¼Œæ— APIé™åˆ¶ï¼‰"""
        try:
            if 'github.com' not in url:
                return None

            parts = url.replace('https://github.com/', '').split('/')
            if len(parts) < 2:
                return None

            owner, repo = parts[0], parts[1]

            headers = {
                'User-Agent': 'Mozilla/5.0'
            }

            # 1. å°è¯•è·å–ä»“åº“ä¸»é¡µï¼ˆç”¨äºæå–æè¿°ï¼‰
            repo_page_url = f"https://github.com/{owner}/{repo}"
            try:
                page_response = requests.get(repo_page_url, headers=headers, timeout=5)
                description = ""
                if page_response.status_code == 200:
                    # ç®€å•æå–æè¿°ï¼ˆåœ¨<meta property="og:description"ä¸­ï¼‰
                    import re
                    desc_match = re.search(r'<meta property="og:description" content="([^"]*)"', page_response.text)
                    if desc_match:
                        description = desc_match.group(1)
            except:
                description = ""

            # 2. ç›´æ¥ä»raw.githubusercontent.comè·å–READMEï¼ˆæ— APIé™åˆ¶ï¼‰
            readme_content = ""

            # å°è¯•å¸¸è§çš„READMEæ–‡ä»¶å
            readme_files = ['README.md', 'readme.md', 'README.MD', 'README', 'README.txt']

            for readme_name in readme_files:
                raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{readme_name}"
                try:
                    readme_response = requests.get(raw_url, headers=headers, timeout=10)
                    if readme_response.status_code == 200:
                        readme_content = readme_response.text[:3000]
                        break
                except:
                    pass

                # å¦‚æœmainåˆ†æ”¯å¤±è´¥ï¼Œå°è¯•masteråˆ†æ”¯
                if not readme_content:
                    raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{readme_name}"
                    try:
                        readme_response = requests.get(raw_url, headers=headers, timeout=10)
                        if readme_response.status_code == 200:
                            readme_content = readme_response.text[:3000]
                            break
                    except:
                        pass

            # ç»„åˆå†…å®¹
            if readme_content or description:
                content = f"Repository: {owner}/{repo}\n"
                if description:
                    content += f"Description: {description}\n"
                if readme_content:
                    content += f"\nREADME (excerpt):\n{readme_content}"
                return content

            return None

        except Exception as e:
            return None

    def call_ai_for_summary(self, url: str, content: str) -> Optional[str]:
        """è°ƒç”¨AIæ¥å£ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹GitHubé¡¹ç›®ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æè¿°ï¼ˆ15-30ä¸ªæ±‰å­—ï¼‰ã€‚
è¦æ±‚ï¼š
1. çªå‡ºé¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½
2. ä½¿ç”¨ä¸“ä¸šæŠ€æœ¯æœ¯è¯­
3. ç®€æ´æ˜äº†ï¼Œä¾¿äºå¿«é€Ÿç†è§£

é¡¹ç›®é“¾æ¥: {url}

é¡¹ç›®ä¿¡æ¯:
{content}

è¯·åªè¿”å›ä¸­æ–‡æè¿°ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

            headers = {"Content-Type": "application/json"}

            # æ ¹æ®ä¸åŒçš„AIæ¥å£æ ¼å¼è°ƒæ•´è¯·æ±‚
            if "anthropic.com" in AI_API_URL.lower():
                payload = {
                    "model": AI_MODEL,
                    "max_tokens": 100,
                    "messages": [{"role": "user", "content": prompt}]
                }
                if AI_API_KEY:
                    headers["x-api-key"] = AI_API_KEY
                    headers["anthropic-version"] = "2023-06-01"

            elif "ollama" in AI_API_URL.lower():
                payload = {
                    "model": AI_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
            else:
                payload = {
                    "model": AI_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": 100
                }
                if AI_API_KEY:
                    headers["Authorization"] = f"Bearer {AI_API_KEY}"

            response = requests.post(AI_API_URL, json=payload, headers=headers, timeout=30)

            if response.status_code == 200:
                result = response.json()

                if "anthropic.com" in AI_API_URL.lower():
                    description = result.get('content', [{}])[0].get('text', '').strip()
                elif "ollama" in AI_API_URL.lower():
                    description = result.get('message', {}).get('content', '').strip()
                else:
                    description = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()

                description = description.strip('"\'').strip()
                return description
            else:
                return None

        except Exception as e:
            return None

    def generate_description(self, url: str) -> Optional[str]:
        """ç”Ÿæˆå•ä¸ªURLçš„æè¿°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            return self.cache[url]

        # è·å–å†…å®¹
        content = self.fetch_github_content(url)
        if not content:
            return None

        # è°ƒç”¨AIç”Ÿæˆæè¿°
        description = self.call_ai_for_summary(url, content)

        if description and len(description) > 5:
            self.cache[url] = description
            return description

        return None


class WeeklyUpdater:
    """å‘¨æŠ¥æ›´æ–°å™¨ - æ›´æ–°å‘¨æŠ¥æ–‡ä»¶ä¸­çš„æè¿°"""

    def __init__(self, weekly_dir: Path):
        self.weekly_dir = weekly_dir

    def extract_links_needing_descriptions(self, file_path: Path) -> List[str]:
        """æå–éœ€è¦æè¿°çš„é“¾æ¥"""
        links = []
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'
        matches = re.findall(pattern, content)

        for _, url, desc in matches:
            if not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc:
                links.append(url)

        return links

    def update_weekly_file(self, file_path: Path, descriptions: Dict[str, str]) -> int:
        """æ›´æ–°å‘¨æŠ¥æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        update_count = 0
        pattern = r'\| \[([^\]]+)\]\((https://[^\)]+)\) \| ([^\|]*) \|'

        def replacer(match):
            nonlocal update_count
            _, url, desc = match.groups()

            if (not desc.strip() or 'æ”¶é›†çš„é¡¹ç›®åœ°å€' in desc) and url in descriptions:
                update_count += 1
                name = url.split('/')[-1]
                return f'| [{name}]({url}) | {descriptions[url]} |'

            return match.group(0)

        updated_content = re.sub(pattern, replacer, content)

        if update_count > 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(updated_content)

        return update_count


class AutoWeeklyProcessor:
    """å®Œå…¨è‡ªåŠ¨åŒ–çš„å‘¨æŠ¥å¤„ç†å™¨"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.generator = WeeklyGenerator(repo_path)
        self.desc_gen = DescriptionGenerator(CACHE_DIR)
        self.updater = WeeklyUpdater(WEEKLY_DIR)

    def process_existing_weeklies(self, max_links_per_week: int = 50):
        """ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“ ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        print("="*60)
        print(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        weekly_files = sorted([
            f for f in os.listdir(str(WEEKLY_DIR))
            if f.startswith('weekly-') and f.endswith('.md')
        ])

        print(f"å‘ç° {len(weekly_files)} ä¸ªå‘¨æŠ¥æ–‡ä»¶\n")

        for i, filename in enumerate(weekly_files, 1):
            file_path = WEEKLY_DIR / filename
            print(f"\n{'='*60}")
            print(f"[{i}/{len(weekly_files)}] å¤„ç†: {filename}")
            print('='*60)

            links = self.updater.extract_links_needing_descriptions(file_path)
            if not links:
                print("  âœ“ å·²å®Œæˆ")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            if len(links) > max_links_per_week:
                print(f"âš ï¸  æœ¬æ¬¡åªå¤„ç†å‰ {max_links_per_week} ä¸ªé“¾æ¥")
                links = links[:max_links_per_week]

            descriptions = {}

            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] å¤„ç†: {url}")
                print(f"    â†’ è·å–GitHubå†…å®¹...")

                desc = self.desc_gen.generate_description(url)

                if desc:
                    print(f"    âœ“ ç”Ÿæˆ: {desc}")
                    descriptions[url] = desc

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        self.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                time.sleep(1)

            self.desc_gen.save_cache()

            if descriptions:
                print(f"\nğŸ“ æ›´æ–°å‘¨æŠ¥æ–‡ä»¶...")
                count = self.updater.update_weekly_file(file_path, descriptions)
                print(f"âœ… æˆåŠŸæ›´æ–° {count} ä¸ªæè¿°åˆ° {filename}")
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        print("="*60)

    def process_all(self, start_date: str = "2025-07-21", max_links_per_week: int = 50):
        """å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸš€ å¯åŠ¨å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆæµç¨‹")
        print("="*60)
        print(f"ğŸ“ ä»“åº“è·¯å¾„: {self.repo_path}")
        print(f"ğŸ¤– AIæ¨¡å‹: {AI_MODEL}")
        print(f"ğŸ“Š æ¯å‘¨æœ€å¤šå¤„ç†: {max_links_per_week} ä¸ªé“¾æ¥\n")

        # æ­¥éª¤1: ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
        generated_files = self.generator.generate_weekly_files(start_date)

        if not generated_files:
            print("\nâš ï¸  æ²¡æœ‰å¯å¤„ç†çš„å‘¨æŠ¥æ–‡ä»¶")
            return

        # æ­¥éª¤2: ä¸ºæ¯ä¸ªå‘¨æŠ¥ç”Ÿæˆæè¿°
        print("\n" + "="*60)
        print("ğŸ“ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆé¡¹ç›®æè¿°å¹¶æ›´æ–°å‘¨æŠ¥")
        print("="*60)

        for i, filename in enumerate(generated_files, 1):
            file_path = WEEKLY_DIR / filename

            print(f"\n{'#'*60}")
            print(f"# [{i}/{len(generated_files)}] å¤„ç†: {filename}")
            print(f"{'#'*60}")

            # æå–éœ€è¦æè¿°çš„é“¾æ¥
            links = self.updater.extract_links_needing_descriptions(file_path)

            if not links:
                print("âœ… æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            # é™åˆ¶å¤„ç†æ•°é‡
            if len(links) > max_links_per_week:
                print(f"âš ï¸  é“¾æ¥è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {max_links_per_week} ä¸ª")
                links = links[:max_links_per_week]

            descriptions = {}

            # å¤„ç†æ¯ä¸ªé“¾æ¥
            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] {url}")
                print(f"    â†’ è·å–GitHubå†…å®¹...")

                description = self.desc_gen.generate_description(url)

                if description:
                    print(f"    âœ“ ç”Ÿæˆ: {description}")
                    descriptions[url] = description

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        self.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)

            # ä¿å­˜ç¼“å­˜
            self.desc_gen.save_cache()

            # æ›´æ–°æ–‡ä»¶
            if descriptions:
                count = self.updater.update_weekly_file(file_path, descriptions)
                print(f"\nâœ… æˆåŠŸæ›´æ–° {count} ä¸ªæè¿°")
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°")

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰å‘¨æŠ¥å¤„ç†å®Œæˆï¼")
        print("="*60)


def main():
    # è®¾ç½®æ§åˆ¶å°ç¼–ç 
    import sys
    if sys.platform == 'win32':
        try:
            import codecs
            sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
        except:
            pass

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         å®Œå…¨è‡ªåŠ¨åŒ–å‘¨æŠ¥ç”Ÿæˆå·¥å…·                              â•‘
â•‘   Gitå†å² â†’ å‘¨æŠ¥ç”Ÿæˆ â†’ AIæè¿° â†’ è‡ªåŠ¨æ›´æ–°                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # æ£€æŸ¥API Key
    if not AI_API_KEY:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
        print("\nå¿«é€Ÿè®¾ç½®ï¼š")
        print("  Windows: $env:ANTHROPIC_API_KEY='your-key'")
        print("  Linux:   export ANTHROPIC_API_KEY='your-key'")
        return

    # é€‰æ‹©æ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("1. å®Œå…¨è‡ªåŠ¨åŒ–ï¼ˆç”Ÿæˆå‘¨æŠ¥ + AIæè¿°ï¼‰")
    print("2. ä»…ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶ï¼ˆä¸ç”Ÿæˆæè¿°ï¼‰")
    print("3. ä»…ä¸ºå·²æœ‰å‘¨æŠ¥ç”Ÿæˆæè¿°")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()

    processor = AutoWeeklyProcessor(GIT_REPO_PATH)

    if choice == "1":
        # å®Œå…¨è‡ªåŠ¨åŒ–
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        max_links = int(input("æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: 50): ").strip() or "50")
        processor.process_all(start_date, max_links)

    elif choice == "2":
        # ä»…ç”Ÿæˆå‘¨æŠ¥
        start_date = input("èµ·å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-21): ").strip() or "2025-07-21"
        processor.generator.generate_weekly_files(start_date)

    elif choice == "3":
        # ä»…ç”Ÿæˆæè¿°
        print("\næ­¤æ¨¡å¼å°†ä¸ºæ‰€æœ‰å·²å­˜åœ¨çš„å‘¨æŠ¥æ–‡ä»¶ç”Ÿæˆæè¿°")
        max_links = int(input("æ¯å‘¨æœ€å¤šå¤„ç†é“¾æ¥æ•° (é»˜è®¤: 50): ").strip() or "50")

        weekly_files = sorted([
            f for f in os.listdir(WEEKLY_DIR)
            if f.startswith('weekly-') and f.endswith('.md')
        ])

        for i, filename in enumerate(weekly_files, 1):
            file_path = WEEKLY_DIR / filename
            print(f"\n{'='*60}")
            print(f"[{i}/{len(weekly_files)}] å¤„ç†: {filename}")
            print('='*60)

            links = processor.updater.extract_links_needing_descriptions(file_path)
            if not links:
                print("âœ“ è¯¥æ–‡ä»¶æ‰€æœ‰é“¾æ¥éƒ½å·²æœ‰æè¿°\n")
                continue

            print(f"ğŸ“Š å‘ç° {len(links)} ä¸ªéœ€è¦æè¿°çš„é“¾æ¥")

            if len(links) > max_links:
                print(f"âš ï¸  æœ¬æ¬¡åªå¤„ç†å‰ {max_links} ä¸ªé“¾æ¥")
                links = links[:max_links]

            descriptions = {}

            for j, url in enumerate(links, 1):
                print(f"\n  [{j}/{len(links)}] å¤„ç†: {url}")
                print(f"    â†’ è·å–GitHubå†…å®¹...")

                desc = processor.desc_gen.generate_description(url)

                if desc:
                    print(f"    âœ“ ç”Ÿæˆ: {desc}")
                    descriptions[url] = desc

                    # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
                    if j % 5 == 0:
                        processor.desc_gen.save_cache()
                        print(f"    ğŸ’¾ å·²ä¿å­˜ç¼“å­˜ ({j}/{len(links)})")
                else:
                    print(f"    âœ— ç”Ÿæˆå¤±è´¥")

                time.sleep(1)

            processor.desc_gen.save_cache()

            if descriptions:
                print(f"\nğŸ“ æ›´æ–°å‘¨æŠ¥æ–‡ä»¶...")
                count = processor.updater.update_weekly_file(file_path, descriptions)
                print(f"âœ… æˆåŠŸæ›´æ–° {count} ä¸ªæè¿°\n")
            else:
                print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æè¿°\n")

    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    main()
